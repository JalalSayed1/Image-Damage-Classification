{"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/JalalSayed1/DL_CW/blob/master/DL_CW_damages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"bun5mMvaeOE3"},"source":["## GUID: 2571964S\n","\n","## to describe and motivate your design choices: architecture, pre-processing, training regime\n","\n","## to analyse, describe and comment on your results\n","\n","## to provide some discussion on what you think are the limitations of your solution and what could be future work"]},{"cell_type":"markdown","metadata":{"id":"sJHqTj3mUn9i"},"source":["Why UNet:\n","1. image generation and segmentation."]},{"cell_type":"markdown","metadata":{"id":"MQTmtA5VUn9i"},"source":["## Can discuss:\n","Choice of Architecture: Explain why you chose a specific architecture over others. For instance, U-Net might be chosen for its efficiency and small dataset effectiveness, while DeepLab could be chosen for its state-of-the-art performance on segmentation tasks.\n","\n","Preprocessing: Describe any preprocessing steps you take, such as resizing images, normalising pixel values, data augmentation, etc.\n","\n","Postprocessing: If you apply any postprocessing to the segmentation maps, such as CRFs to sharpen the boundaries, explain why and how this improves the results.\n","\n","Loss Functions: Discuss the choice of loss function, which in the case of segmentation could be cross-entropy, dice coefficient, or a combination of several loss functions.\n","\n","Metrics: Describe the metrics you'll use to evaluate the performance of your model, such as pixel accuracy, mean Intersection over Union (IoU), etc.\n","\n","Training Strategy: Detail the training process, including the choice of optimiser, learning rate, batch size, and any other hyperparameters.\n","\n","Results and Analysis: Present the results and provide an analysis of what worked and what didnâ€™t. Discuss any challenges you faced and how you addressed them.\n","\n","Visualisation: Explain the importance of visualisation in understanding the performance of your model. For example, overlay images help to see where the model is performing well and where it is making mistakes."]},{"cell_type":"markdown","metadata":{"id":"O2H6z8b9eYdY"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"fE4PJVsod6A3"},"source":["# Damages - Deep Learning Coursework 2024\n","\n","The aim of this coursework will be for you to design, implement and test a deep learning architecture to detect and identify damage in images. Digitization allows to make historical pictures and art much more widely available to the public. Many such pictures have suffered some form of damage due to time, storage conditions and the fragility of the original medium. For example, the image below (A) shows an example of a digitized parchment that has suffered significant damage over time.\n","\n","**The aim of this project is for you to design, implement and evaluate a deep learning model to detect and identify damage present in images.**\n","\n","<table>\n","<tr>\n","<td>\n","<div>\n","<img src=\"damage_data/image_path/cljmrkz5n341f07clcujw105j.png\" width=\"500\"/>\n","</div>\n","</td>\n","<td>\n","<div>\n","<img src=\"damage_data/annotation_rgb_path/cljmrkz5n341f07clcujw105j.png\" width=\"500\"/>\n","</div>\n","</td>\n","</tr>\n","<td><center>(A) Image</center></td><td><center>(B) damage labels</center></td>\n","</table>\n","*(Note that the images will only show once you have downloaded the dataset)*\n","\n","\n","The image labels in this figure (B) identifies a smatter of peeling paint, a large stained area in the bottom left and a missing part on the top left. Each colour in those images corresponds to a different category of damage, including `fold`, `writing` or `burn marks`. You are provided with a dataset of a variety of damaged images, from Parchment to ceramic or wood painting, and detailed annotations of a range of damages.\n","\n","You are free to use any architecture you prefer, from what we have seen in class. You can decide to use unsupervised pre-training of only supervised end-to-end training - the approach you choose is your choice.\n","\n","### Hand-in date: Friday 15th of March before 4:30pm (on Moodle)\n","\n","### Steps & Hints\n","* First, look at the data. What are the different type of images (content), what type of material, what type of damage? How different are they? What type of transformations for your data augmentation do you think would be acceptable here?.\n","* Second, check the provided helper functions for loading the data and separate into training and test set and cross-validation.\n","* Design a network for the task. What output? What layers? How many? Do you want to use an Autoencoder for unsupervised pre-training?\n","* Choose a loss function for your network\n","* Select optimiser and training parameters (batch size, learning rate)\n","* Optimise your model, and tune hyperparameters (especially learning rate, momentum etc)\n","* Analyse the results on the test data. How to measure success? Which classes are recognised well, which are not? Is there confusion between some classes? Look at failure cases.\n","* If time allows, go back to drawing board and try a more complex, or better, model.\n","* Explain your thought process, justify your choices and discuss the results!\n","\n","### Submission\n","* submit ONE zip file on Moodle containing:\n","  * **your notebook**: use `File -> download .ipynb` to download the notebook file locally from colab.\n","  * **a PDF file** of your notebook's output as you see it: use `File -> print` to generate a PDF.\n","* your notebook must clearly contains separate cells for:\n","  * setting up your model and data loader\n","  * training your model from data\n","  * loading your pretrained model from github/gitlab/any other online storage you like!\n","  * testing your model on test data.\n","* The training cells must be disabled by a flag, such that when running *run all* on your notebook it does\n","  * load the data\n","  * load your model\n","  * apply the model to the test data\n","  * analyse and display the results and accuracy\n","* In addition provide markup cell:\n","  * containing your student number at the top\n","  * to describe and motivate your design choices: architecture, pre-processing, training regime\n","  * to analyse, describe and comment on your results\n","  * to provide some discussion on what you think are the limitations of your solution and what could be future work\n","\n","* **Note that you must put your trained model online so that your code can download it.**\n","\n","\n","### Assessment criteria\n","* In order to get a pass mark, you will need to demonstrate that you have designed and trained a deep NN to solve the problem, using sensible approach and reasonable efforts to tune hyper-parameters. You have analysed the results. It is NOT necessary to have any level of accuracy (a network that predicts poorly will always yield a pass mark if it is designed, tuned and analysed sensibly).\n","* In order to get a good mark, you will show good understanding of the approach and provide a working solution.\n","* in order to get a high mark, you will demonstrate a working approach of gradual improvement between different versions of your solution.\n","* bonus marks for attempting something original if well motivated - even if it does not yield increased performance.\n","* bonus marks for getting high performance, and some more points are to grab for getting the best performance in the class.\n","\n","### Notes\n","* You are provided code to isolate the test set and cross validation, make sure to keep the separation clean to ensure proper setting of all hyperparameters.\n","* I recommend to start with small models that can be easier to train to set a baseline performance before attempting more complex one.\n","* Be mindful of the time!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1962,"status":"ok","timestamp":1709897797674,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"W0KpHbRKqhn_","outputId":"346ece0b-47e1-4c7b-9b49-c60275294d36"},"outputs":[],"source":["using_colab = False\n","if using_colab:\n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ZQa07mdpd6A6"},"source":["## Housekeeping"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5239,"status":"ok","timestamp":1709897802911,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"GtWZa-JPesz7","outputId":"00c96bee-32b0-48d5-9f9d-8048ee225c5f"},"outputs":[],"source":["!pip install gdown pytorch_lightning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1709897802911,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"FK_rOEmQuwV3","outputId":"80d14767-34c0-497a-fe11-d4a3a8619c5b"},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n","\n","import os\n","import pandas as pd\n","import PIL\n","PIL.Image.MAX_IMAGE_PIXELS = 243748701\n","from PIL import Image\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import gdown\n","import shutil\n","\n","DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","DEVICE"]},{"cell_type":"markdown","metadata":{"id":"CU2Ky6meo0kt"},"source":["# Load dataset"]},{"cell_type":"markdown","metadata":{"id":"aLMiDRyUd6A8"},"source":["We then load the metadata in a dataframe for convenience"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1709897802911,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"6nM_cNHrgsjY","outputId":"b9bf71c1-306c-4073-8f24-245d1e6ba985"},"outputs":[],"source":["if using_colab:\n","    !pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709897802911,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"n_T3a3WHiL8P"},"outputs":[],"source":["download_dataset = False\n","if download_dataset:\n","  if not os.path.exists(\"damage_data\"):\n","      # !gdown 1v8aUId0-tTW3ln3O2BE4XajQeCToOEiS -O damages.zip\n","\n","      # FILE_ID = \"1771AIemjPvrIGjf_87tGMqwRhxAUuuw-\"\n","      # !wget 'https://drive.google.com/uc?id=1771AIemjPvrIGjf_87tGMqwRhxAUuuw-&export=download' -O damages.zip\n","\n","      !cp -r \"/content/drive/MyDrive/Colab Notebooks/CW/damages.zip\" \".\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709897802911,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"0S7fMugEd6A8"},"outputs":[],"source":["dataset_exist = os.path.exists(\"damage_data\")\n","if dataset_exist:\n","  # set  that to wherever you want to store the data (eg, your Google Drive), choose a persistent location!\n","  root_dir = '.'\n","  # root_dir = '/content/drive/MyDrive/Colab Notebooks/CW/'\n","\n","  data_dir = os.path.join(root_dir, \"damage_data\")\n","  csv_path = os.path.join(data_dir, 'metadata.csv')\n","\n","  zip_path = os.path.join(root_dir, 'damages.zip')\n","\n","  try:\n","      if not dataset_exist:\n","        # if .zip file is not unpacked, unpack it\n","        shutil.unpack_archive(zip_path, root_dir)\n","      df = pd.read_csv(csv_path)\n","\n","  except Exception as e:  # if the dataset has not been downloaded yet, do it.\n","    if download_dataset:\n","        zip_path = os.path.join(root_dir, 'damages.zip')\n","        gdown.download(id='1v8aUId0-tTW3ln3O2BE4XajQeCToOEiS', output=zip_path)\n","        shutil.unpack_archive(zip_path, root_dir)\n","        df = pd.read_csv(csv_path)\n","    print(e)"]},{"cell_type":"markdown","metadata":{"id":"18VmATnob3G_"},"source":["This dataframe has the paths of where the dataset images and annotation labels are stored, plus classification labels."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709897802911,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"obKQZoPbd6A8"},"outputs":[],"source":["if dataset_exist:\n","  df"]},{"cell_type":"markdown","metadata":{"id":"0DMf6xlKd6A9"},"source":["The images in the dataset are categorised in terms of the type of `material`, meaning what was the original picture on, eg, Parchment, Glass or Textile."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1709897802912,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"HyWU2V3Dd6A9"},"outputs":[],"source":["if dataset_exist:\n","  df['material'].unique()"]},{"cell_type":"markdown","metadata":{"id":"UZ12Mf2Ud6A9"},"source":["Moreover, images are also categorised in terms on the `content` of the image, meaning what is depicted: eg, Line art, geometric patterns, etc."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1709897802912,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"d1SzCR0qd6A9"},"outputs":[],"source":["if dataset_exist:\n","  df['content'].unique()"]},{"cell_type":"markdown","metadata":{"id":"K8rH3KykdMNc"},"source":["## Labels\n","Segmentation labels are saved as a PNG image, where each number from 1 to 15 corresponds to a damage class like Peel, Scratch etc; the Background class is set to 255, and the Clean class (no damage) is set to 0. We also provide code to convert these annotation values to RGB colours for nicer visualisation, but for training you should use the original annotations."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":475,"status":"ok","timestamp":1709897803381,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"Td8pW0M6WLZO","outputId":"d4b784ed-e523-4138-bf8f-c7c0e4091d65"},"outputs":[],"source":["name_color_mapping = {\n","    \"Material loss\": \"#1CE6FF\",\n","    \"Peel\": \"#FF34FF\",\n","    \"Dust\": \"#FF4A46\",\n","    \"Scratch\": \"#008941\",\n","    \"Hair\": \"#006FA6\",\n","    \"Dirt\": \"#A30059\",\n","    \"Fold\": \"#FFA500\",\n","    \"Writing\": \"#7A4900\",\n","    \"Cracks\": \"#0000A6\",\n","    \"Staining\": \"#63FFAC\",\n","    \"Stamp\": \"#004D43\",\n","    \"Sticker\": \"#8FB0FF\",\n","    \"Puncture\": \"#997D87\",\n","    \"Background\": \"#5A0007\",\n","    \"Burn marks\": \"#809693\",\n","    \"Lightleak\": \"#f6ff1b\",\n","}\n","\n","class_names = [ 'Material loss', 'Peel', 'Dust', 'Scratch',\n","                'Hair', 'Dirt', 'Fold', 'Writing', 'Cracks', 'Staining', 'Stamp',\n","                'Sticker', 'Puncture', 'Burn marks', 'Lightleak', 'Background']\n","\n","class_to_id = {class_name: idx+1 for idx, class_name in enumerate(class_names)}\n","class_to_id['Background'] = 255  # Set the Background ID to 255\n","\n","def hex_to_rgb(hex_color: str) -> tuple:\n","    hex_color = hex_color.lstrip('#')\n","    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n","\n","id_to_rgb = {class_to_id[class_name]: hex_to_rgb(color) for class_name, color in name_color_mapping.items()}\n","id_to_rgb[0] = (0,0,0)\n","\n","# Create id2label mapping: ID to class name\n","id2label = {idx: class_name for class_name, idx in class_to_id.items()}\n","\n","# Create label2id mapping: class name to ID, which is the same as class_to_id\n","label2id = class_to_id\n","\n","# Non-damaged pixels\n","id2label[0] = 'Clean'\n","label2id['Clean'] = 0\n","\n","print(\"id_to_rgb\")\n","[print(f\"{k}: {v}\") for k, v in id_to_rgb.items()]\n","print()\n","print(\"id2label\")\n","[print(f\"{k}: {v}\") for k, v in id2label.items()]\n","print()\n","print(\"label2id\")\n","[print(f\"{k}: {v}\") for k, v in label2id.items()]\n","print()\n","print(\"name_color_mapping\")\n","[print(f\"{k}: {v}\") for k, v in name_color_mapping.items()]\n","print()\n","print(\"class_to_id\")\n","[print(f\"{k}: {v}\") for k, v in class_to_id.items()]\n","\n","print()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":343},"executionInfo":{"elapsed":49,"status":"ok","timestamp":1709897803381,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"SoOFtDuud6A-","outputId":"d03fb847-cbc5-4b6e-f0b0-acad2f1a13da"},"outputs":[],"source":["from IPython.display import Markdown\n","\n","legend='#### Colour labels for each damage type\\n'\n","for damage in class_names:\n","    legend += '- <span style=\"color: {color}\">{damage}</span>.\\n'.format(color=name_color_mapping[damage], damage=damage)\n","display(Markdown(legend))"]},{"cell_type":"markdown","metadata":{"id":"Oantk2PzcEFb"},"source":["## Create dataset splits\n","\n","Here is an example of how to split the dataset for Leave-one-out cross validation (LOOCV) based on material."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":41,"status":"ok","timestamp":1709897803381,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"ay2LWSuLiDom"},"outputs":[],"source":["def create_leave_one_out_splits(df, criterion='material'):\n","\n","    grouped = df.groupby(criterion)\n","    content_splits = {name: group for name, group in grouped}\n","    unique_val = df[criterion].unique()\n","\n","    # Initialize a dictionary to hold the train and validation sets for each LOOCV iteration\n","    loocv_splits = {}\n","\n","    for value in unique_val:\n","        # Create the validation set\n","        val_set = content_splits[value]\n","\n","        # Create the training set\n","        train_set = pd.concat([content_splits[c] for c in unique_val if c != value])\n","\n","        # Add these to the loocv_splits dictionary\n","        loocv_splits[value] = {'train_set': train_set, 'val_set': val_set}\n","\n","    return loocv_splits\n"]},{"cell_type":"markdown","metadata":{"id":"5y5NruT7d6A-"},"source":["For this coursework, we will want to assess the generalisation of the method, so for that we will keep one type of material (`Canvas`) as test set, and only train on the remaining ones."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42,"status":"ok","timestamp":1709897803383,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"KNf29AtXd6A-","outputId":"04e10367-c636-4417-d3d5-b6ffdc528d29"},"outputs":[],"source":["# split the dataset according to material type\n","full_splits = create_leave_one_out_splits(df, 'material')\n","\n","# use Canvas as test set\n","test_set = full_splits['Canvas']['val_set']\n","\n","# use the rest as training set\n","train_set = full_splits['Canvas']['train_set']\n","\n","# prepare a leave-one-out cross validation for the training set\n","loocv_splits = create_leave_one_out_splits(train_set, 'material')\n","\n","# identify the different type of image content\n","unique_material = train_set['material'].unique()\n","\n","print(\"Training set materials:\", unique_material)\n","print(\"Test set material:\", test_set['material'].unique())\n"]},{"cell_type":"markdown","metadata":{"id":"sJRkv1lXcNFq"},"source":["To help you, here are some helper functions to help crop and process images."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1709897803383,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"m9bJdsQGd6A_"},"outputs":[],"source":["def random_square_crop_params(image, target_size):\n","    width, height = image.size\n","    min_edge = min(width, height)\n","\n","    # Conditionally set the range for random crop size\n","    lower_bound = min(min_edge, target_size)\n","    upper_bound = max(min_edge, target_size)\n","\n","    # Generate crop_size\n","    crop_size = random.randint(lower_bound, upper_bound)\n","\n","    # Check and adjust if crop_size is larger than any dimension of the image\n","    if crop_size > width or crop_size > height:\n","        crop_size = min(width, height)\n","\n","    # Generate random coordinates for the top-left corner of the crop\n","    x = random.randint(0, width - crop_size)\n","    y = random.randint(0, height - crop_size)\n","\n","    return (x, y, x + crop_size, y + crop_size)\n","\n","def apply_crop_and_resize(image, coords, target_size):\n","    image_crop = image.crop(coords)\n","    image_crop = image_crop.resize((target_size, target_size), Image.NEAREST)\n","    return image_crop"]},{"cell_type":"markdown","metadata":{"id":"AOlkN-lVd6A_"},"source":["We also provide a simple class for holding the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1709897803383,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"qBjlhUmnd6A_"},"outputs":[],"source":["import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","import random\n","import numpy as np\n","from PIL import Image\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, dataframe, target_size, is_train=True):\n","        self.dataframe = dataframe\n","        self.target_size = target_size\n","        self.is_train = is_train\n","\n","        self.to_tensor = transforms.ToTensor()\n","\n","        # Define the normalization transform\n","        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                              std=[0.229, 0.224, 0.225])\n","\n","    def __len__(self):\n","            return len(self.dataframe)\n","\n","    def __getitem__(self, idx):\n","        row = self.dataframe.iloc[idx]\n","        image = Image.open(row['image_path']).convert('RGB')\n","        annotation = Image.open(row['annotation_path']).convert('L')\n","        annotation_rgb = Image.open(row['annotation_rgb_path']).convert('RGB')\n","        id = row['id']\n","        material = row['material']\n","        content = row['content']\n","\n","        if self.is_train:\n","            # Generate random square cropping coordinates\n","            crop_coords = random_square_crop_params(image, self.target_size)\n","\n","            # Apply the same cropping and resizing to all\n","            image = apply_crop_and_resize(image, crop_coords, self.target_size)\n","            annotation = apply_crop_and_resize(annotation, crop_coords, self.target_size)\n","            annotation_rgb = apply_crop_and_resize(annotation_rgb, crop_coords, self.target_size)\n","        else:  # Validation\n","            # Instead of cropping, downsize the images so that the longest edge is 1024 or less\n","            # max_edge = max(image.size)\n","            # if max_edge > 1024:\n","            #     downsample_ratio = 1024 / max_edge\n","            #     new_size = tuple([int(dim * downsample_ratio) for dim in image.size])\n","\n","            #     image = image.resize(new_size, Image.BILINEAR)\n","            #     annotation = annotation.resize(new_size, Image.NEAREST)\n","            #     annotation_rgb = annotation_rgb.resize(new_size, Image.BILINEAR)\n","\n","            # Generate random square cropping coordinates\n","            crop_coords = random_square_crop_params(image, self.target_size)\n","\n","            # Apply the same cropping and resizing to all\n","            image = apply_crop_and_resize(image, crop_coords, self.target_size)\n","            annotation = apply_crop_and_resize(annotation, crop_coords, self.target_size)\n","            annotation_rgb = apply_crop_and_resize(annotation_rgb, crop_coords, self.target_size)\n","\n","        # Convert PIL images to PyTorch tensors\n","        image = self.to_tensor(image)\n","        annotation = torch.tensor(np.array(annotation), dtype=torch.long)\n","        annotation_rgb = self.to_tensor(annotation_rgb)\n","\n","        # Normalize the image\n","        image = self.normalize(image)\n","\n","        # Change all values in annotation that are 255 to 16\n","        #! why?\n","        annotation[annotation == 255] = 16\n","\n","        return {\n","            'image': image,\n","            'annotation': annotation,\n","            'annotation_rgb': annotation_rgb,\n","            'id': id,\n","            'material': material,\n","            'content': content\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"mvI-9NaxcljF"},"source":["Here we create a DataModule which encapsulates our training and validation DataLoaders; you can also do this manually by only using the Pytorch DataLoader class, lines 24 and 27."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1709897803383,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"SPuGFmPGuIFk"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torchvision import transforms\n","import torch.nn.functional as F\n","\n","class CustomDataModule(pl.LightningDataModule):\n","    def __init__(self, loocv_splits, current_material, target_size, batch_size=32, num_workers=4):\n","        super().__init__()\n","        self.loocv_splits = loocv_splits\n","        self.current_material = current_material\n","        self.target_size = target_size\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","\n","    def prepare_data(self):\n","        pass\n","\n","    def setup(self, stage=None):\n","        # Load current train and validation set based on LOOCV iteration\n","        train_df = self.loocv_splits[self.current_material]['train_set']\n","        val_df = self.loocv_splits[self.current_material]['val_set'].sample(frac=1).reset_index(drop=True)\n","\n","        self.train_dataset = CustomDataset(dataframe=train_df, target_size=self.target_size, is_train=True)\n","        self.val_dataset = CustomDataset(dataframe=val_df, target_size=self.target_size, is_train=False)\n","\n","    def train_dataloader(self):\n","        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n","\n","    def val_dataloader(self):\n","        return DataLoader(self.val_dataset, batch_size=1, shuffle=False, num_workers=self.num_workers)\n","\n","    def test_dataloader(self):\n","        pass\n"]},{"cell_type":"markdown","metadata":{"id":"KR1Qgc4Sd6A_"},"source":["The following will create a data module for validating on the first content in the list (`Parchment`) and training on all the other types of material (you will want to do that for each fold)."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24,"status":"ok","timestamp":1709897803384,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"MEIvJAAuwKBb"},"outputs":[],"source":["num_workers = 4 if using_colab else 0\n","data_module = CustomDataModule(loocv_splits=loocv_splits,\n","                               current_material=unique_material[0],\n","                               target_size=512,\n","                               batch_size=4,\n","                               num_workers=num_workers)"]},{"cell_type":"markdown","metadata":{"id":"CkEbiloVd6BA"},"source":["Finally, we can get the train and validation data loaders from the data module."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":897,"status":"ok","timestamp":1709897804257,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"E2nVOsBIwZPq","outputId":"c971f852-d47f-40be-d77e-97157d78088f"},"outputs":[],"source":["if dataset_exist:\n","  data_module.setup()\n","  train_loader = data_module.train_dataloader()\n","  val_loader = data_module.val_dataloader()\n","\n","  print(\"Number of training batches:\", len(train_loader))\n","  print(\"Number of training samples:\", len(train_loader.dataset))\n","  # val dataset is set to have batch size of 1:\n","  print(\"Number of validation batches:\", len(val_loader))\n","  print(\"Number of validation samples:\", len(val_loader.dataset))\n","  print(\"image size:\", train_loader.dataset[-1]['image'].shape)\n","  print(\"annotation size:\", train_loader.dataset[-1]['annotation'].shape)\n","  print(\"number of material in training set:\", len(train_loader.dataset.dataframe['material'].unique()))\n"]},{"cell_type":"markdown","metadata":{"id":"MevlLaqfc-XA"},"source":["# Dataset visualisation"]},{"cell_type":"markdown","metadata":{"id":"xShAlBKvc32-"},"source":["We need to denormalise the images so we can display them"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1709897804257,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"xHT-CU9tyblk"},"outputs":[],"source":["# Mean and std used for normalization\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","\n","def denormalize(image, mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]):\n","    img_cpy = image.copy()\n","    for i in range(3):\n","        img_cpy[..., i] = img_cpy[..., i] * std[i] + mean[i]\n","    return img_cpy"]},{"cell_type":"markdown","metadata":{"id":"bw2zLzOJdBfb"},"source":["## Visualise training samples\n","Random square crops of the images and correspoding RGB annotations on their own and overlaid onto the image."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":934},"executionInfo":{"elapsed":11778,"status":"ok","timestamp":1709897816029,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"zDlIgX7ZwdEr","outputId":"551e8081-94ba-4fa0-ce7d-bfadef225973"},"outputs":[],"source":["num_to_visualise = len(train_loader.dataset) # all images\n","print(\"Number of image that can be visualized:\", num_to_visualise)\n","\n","example_batch = next(iter(train_loader))\n","print(\"Shape of the image batch:\", example_batch['image'].shape)\n","\n","example_images = example_batch['image']\n","example_annotations = example_batch['annotation']\n","example_annotation_rgbs = example_batch['annotation_rgb']\n","\n","# Number of examples to visualize\n","# N = min(4, len(example_images))\n","N = min(num_to_visualise, len(example_images))\n","print(\"Number of examples to visualize:\", N)\n","\n","fig, axes = plt.subplots(N, 3, figsize=(15, 5 * N))\n","\n","for ax, col in zip(axes[0], ['Image', 'Annotation', 'Overlay']):\n","    ax.set_title(col, fontsize=24)\n","\n","for i in range(N):\n","    example_image = denormalize(example_images[i].numpy().transpose((1, 2, 0)), mean, std)  # C, H, W -> H, W, C\n","    example_annotation = Image.fromarray(np.uint8(example_annotations[i].numpy()), 'L')\n","    example_annotation_rgb = example_annotation_rgbs[i].numpy().transpose((1, 2, 0))  # C, H, W -> H, W, C\n","\n","    # Create an alpha (transparency) channel where black pixels in annotation_rgb are fully transparent\n","    alpha_channel = np.all(example_annotation_rgb == [0, 0, 0], axis=-1)\n","    example_annotation_rgba = np.dstack((example_annotation_rgb, np.where(alpha_channel, 0, 1)))\n","\n","    axes[i, 0].imshow(example_image)\n","    axes[i, 0].axis('off')\n","\n","    #axes[i, 1].imshow(example_annotation, cmap='gray', vmin=0, vmax=255)\n","    axes[i, 1].imshow(example_annotation_rgb)\n","    axes[i, 1].axis('off')\n","\n","    axes[i, 2].imshow(example_image)\n","    axes[i, 2].imshow(example_annotation_rgba)\n","    axes[i, 2].axis('off')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"t71C8ZeXdyWS"},"source":["Visualising the validation set, which loads the left-out class as whole images."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6348,"status":"ok","timestamp":1709897822364,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"anaTYaT6RNvH","outputId":"8074be3f-f3d4-4deb-c4a7-caeabbaf04be"},"outputs":[],"source":["val_iter = iter(val_loader)\n","example_batches = [next(val_iter) for _ in range(N)]\n","\n","# Initialize empty lists to collect different parts of each batch\n","example_images = []\n","example_annotations = []\n","example_annotation_rgbs = []\n","example_materials = []\n","example_contents = []\n","\n","# Populate the lists with the data from the 4 batches\n","for batch in example_batches:\n","    example_images.append(batch['image'].squeeze())\n","    example_annotations.append(batch['annotation'].squeeze())\n","    example_annotation_rgbs.append(batch['annotation_rgb'].squeeze())\n","    example_materials.append(batch['material'][0])\n","    example_contents.append(batch['content'][0])\n","\n","    print(\"batch image shape:\", batch['image'].shape)\n","    print(\"batch annotation shape:\", batch['annotation'].shape)\n","    print(\"Shape of the image batch:\", example_images[0].shape)\n","    print(\"Shape of the annotation batch:\", example_annotations[0].shape)\n","\n","# Number of examples to visualize\n","# N = min(4, len(example_images))\n","N = min(num_to_visualise, len(example_images))\n","\n","fig, axes = plt.subplots(N, 3, figsize=(15, 5 * N))\n","\n","for ax, col in zip(axes[0], ['Image', 'Annotation', 'Overlay']):\n","    ax.set_title(col, fontsize=24)\n","\n","for i in range(N):\n","    example_image = denormalize(example_images[i].numpy().transpose((1, 2, 0)), mean, std)  # C, H, W -> H, W, C\n","    example_annotation = example_annotations[i].numpy()\n","    example_annotation_rgb = example_annotation_rgbs[i].numpy().transpose((1, 2, 0))  # C, H, W -> H, W, C\n","    example_material = example_materials[i]\n","    example_content = example_contents[i]\n","    # Create an alpha (transparency) channel where black pixels in annotation_rgb are fully transparent\n","    alpha_channel = np.all(example_annotation_rgb == [0, 0, 0], axis=-1)\n","    example_annotation_rgba = np.dstack((example_annotation_rgb, np.where(alpha_channel, 0, 1)))\n","    axes[i, 0].imshow(example_image)\n","    axes[i, 0].axis('off')\n","\n","    axes[i, 1].imshow(example_annotation_rgb)\n","    axes[i, 1].axis('off')\n","\n","    axes[i, 2].imshow(example_image)\n","    axes[i, 2].imshow(example_annotation_rgba)\n","    axes[i, 2].axis('off')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"aJ8iDqqod6BB"},"source":["# Evaluation\n","\n","For the final evaluation of the model, make sure to test performance on the left out category, `Canvas` to have a fair idea on how well the model generalises."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1709897822364,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"556T0aPid6BB"},"outputs":[],"source":["test_module = CustomDataModule(loocv_splits=full_splits,\n","                               current_material='Canvas',\n","                               target_size=512,\n","                               batch_size=4)\n","\n","test_module.setup()\n","\n","test_loader = test_module.val_dataloader()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# print(test_loader.dataset[0]['annotation'].shape)\n","# max_all_images = torch.max(torch.tensor([torch.max(test_loader.dataset[i]['annotation']) for i in range(len(test_loader.dataset))]))\n","# print(max_all_images)\n","# min_all_images = torch.min(torch.tensor([torch.min(test_loader.dataset[i]['annotation']) for i in range(len(test_loader.dataset))]))\n","# print(min_all_images)"]},{"cell_type":"markdown","metadata":{"id":"cQNxUQ4qUn_5"},"source":["---\n","\n","# My Solution:\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"s1D8xTxLUn_5"},"source":["### Network Design"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":220,"status":"ok","timestamp":1709898721246,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"K8vFOgiPUn_5"},"outputs":[],"source":["# class UNet(nn.Module):\n","#     def __init__(self, in_channels, out_channels, features=[8, 16, 32, 64, 128, 256, 512, 1024, 2048], verbose=False):\n","#         super().__init__()\n","#         self.verbose = verbose\n","#         self.in_channels = in_channels\n","#         self.out_channels = out_channels\n","#         self.features = features\n","        \n","#         self.upsample = nn.Sequential(nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","#                                       # nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n","#                         )\n","\n","#         self.in_conv = nn.Sequential(\n","#             nn.Conv2d(in_channels, features[0], kernel_size=3, padding=1),\n","#             nn.ReLU()\n","#             # nn.Conv2d(features[0], features[0], kernel_size=3, padding=1),\n","#             # nn.ReLU()\n","#         )\n","\n","#         self.encoder_layers = nn.ModuleList()\n","#         self.decoder_layers = nn.ModuleList()\n","\n","#         #' Create encoder path\n","#         prev_channels = features[0]\n","\n","#         for index, feature in enumerate(features[1:]):\n","#             self.encoder_layers.append(self.create_encoder_layer(prev_channels, feature))\n","#             if (index + 1) % 2 == 0:\n","#                 # Reduce the spatial dimensions by half every 2 layers:\n","#                 self.encoder_layers.append(nn.MaxPool2d(2)) \n","#             prev_channels = feature\n","\n","#         #' Create decoder path\n","#         for index, feature in enumerate(features[::-1][:-1]):\n","#             # + feature because of the skip connection cat operation:\n","#             in_channels = prev_channels*2 if index == 0 else prev_channels + features[::-1][index]\n","#             # feature * 2 # if index > 0 else feature\n","#             self.decoder_layers.append(self.create_decoder_layer(prev_channels, feature))\n","#             if (index + 1) % 2 == 0:\n","#                 # upsample by x2:\n","#                 self.decoder_layers.append(self.upsample)\n","#             prev_channels = feature * 2\n","\n","#         # Final layer of the decoder\n","#         self.final_layer = nn.Sequential(\n","#             nn.Conv2d(prev_channels//2, out_channels, kernel_size=3, stride=2, padding=1)\n","#         )\n","\n","#     def create_encoder_layer(self, in_channels, out_channels):\n","#         return nn.Sequential(\n","#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","#             nn.BatchNorm2d(out_channels), # normalize the output of the previous layer for faster training\n","#             nn.ReLU()\n","#             # nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","#             # nn.BatchNorm2d(out_channels),\n","#             # nn.ReLU()\n","#         )\n","\n","#     def create_decoder_layer(self, in_channels, out_channels):\n","#         return nn.Sequential(\n","#             nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","#             nn.BatchNorm2d(out_channels),\n","#             nn.ReLU()\n","#             # nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","#             # nn.ReLU()\n","#         )\n","\n","#     def enable_verbose(self, enabled):\n","#         self.verbose = enabled\n","\n","#     def forward(self, x):\n","        \n","#         model_dtypes = [param.dtype for param in self.parameters()]\n","#         if any([model_dtype == torch.float16 for model_dtype in model_dtypes]):\n","#           if self.verbose:\n","#               print(f\"Using {model_dtypes[0]} percision..\")\n","#           x = x.to('cuda', dtype=torch.float16)\n","#         elif self.verbose:\n","#           print(f\"Using {model_dtypes[0]} percision..\")\n","\n","#         if self.verbose:\n","#             print(f\"Input shape: {x.shape}, dtype: {x.dtype }\")\n","\n","#         # Store the output from each encoder layer for use in the decoder path (skip connections):\n","#         encoder_outputs = []\n","\n","#         x = self.in_conv(x)\n","#         encoder_outputs.append(x)\n","#         if self.verbose:\n","#             print(f\"X shape after in_conv: {x.shape}\")\n","\n","#         # Apply encoder layers\n","#         for encoder_layer in self.encoder_layers:\n","#             x = encoder_layer(x)\n","#             encoder_outputs.append(x)\n","#             if self.verbose:\n","#                 print(f\"X shape after encoder layer: {x.shape}\")\n","\n","#         # upsample x to match the size decoder input:\n","#         # if self.verbose:\n","#         #     print(f\"Upsampling x by 2..\")\n","#         # x = self.upsample(x)\n","\n","#         # Apply decoder layers\n","#         for index, decoder_layer in enumerate(self.decoder_layers):\n","#             # apply the decoder layer:\n","#             x = decoder_layer(x)\n","#             if self.verbose:\n","#                 print(f\"X shape after decoder layer: {x.shape}\")\n","            \n","#             # get the corresponding encoder output:\n","#             encoder_output = encoder_outputs.pop()\n","#             if self.verbose:\n","#                 print(f\"Concatenating: x: {x.shape}, encoder_output: {encoder_output.shape}\")\n","#             x = torch.cat((x, encoder_output), dim=1)\n","#             if self.verbose:\n","#                 print(f\"X shape after concatenation: {x.shape}\")\n","\n","#         x = self.final_layer(x)\n","#         if self.verbose:\n","#             print(f\"X shape after final layer: {x.shape}\")\n","\n","#         return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\" Parts of the U-Net model \"\"\"\n","#' Resource: https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_model.py\n","\n","class DoubleConv(nn.Module):\n","    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, mid_channels=None):\n","        super().__init__()\n","        if not mid_channels:\n","            mid_channels = out_channels\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n","            nn.BatchNorm2d(mid_channels),\n","            nn.ReLU(inplace=True)\n","            # nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n","            # nn.BatchNorm2d(out_channels),\n","            # nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","class Down(nn.Module):\n","    \"\"\"Downscaling with maxpool then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.maxpool_conv = nn.Sequential(\n","            nn.MaxPool2d(2),\n","            DoubleConv(in_channels, out_channels)\n","        )\n","\n","    def forward(self, x):\n","        return self.maxpool_conv(x)\n","\n","class Up(nn.Module):\n","    \"\"\"Upscaling then double conv\"\"\"\n","\n","    def __init__(self, in_channels, out_channels, bilinear=True):\n","        super().__init__()\n","\n","        # if bilinear, use the normal convolutions to reduce the number of channels\n","        if bilinear:\n","            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n","        else:\n","            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n","            self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        # input is CHW\n","        diffY = x2.size()[2] - x1.size()[2]\n","        diffX = x2.size()[3] - x1.size()[3]\n","\n","        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n","                        diffY // 2, diffY - diffY // 2])\n","        # if you have padding issues, see\n","        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n","        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n","        x = torch.cat([x2, x1], dim=1)\n","        return self.conv(x)\n","\n","class OutConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(OutConv, self).__init__()\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        return self.conv(x)\n","    \n","class UNet(nn.Module):\n","    def __init__(self, n_channels, n_classes, bilinear=False):\n","        super(UNet, self).__init__()\n","        self.n_channels = n_channels\n","        self.n_classes = n_classes\n","        self.bilinear = bilinear\n","\n","        self.inc = (DoubleConv(n_channels, 64))\n","        self.down1 = (Down(64, 128))\n","        self.down2 = (Down(128, 256))\n","        self.down3 = (Down(256, 512))\n","        factor = 2 if bilinear else 1\n","        self.down4 = (Down(512, 1024 // factor))\n","        self.up1 = (Up(1024, 512 // factor, bilinear))\n","        self.up2 = (Up(512, 256 // factor, bilinear))\n","        self.up3 = (Up(256, 128 // factor, bilinear))\n","        self.up4 = (Up(128, 64, bilinear))\n","        self.outc = (OutConv(64, n_classes))\n","\n","    def forward(self, x):\n","        \n","        model_dtypes = [param.dtype for param in self.parameters()]\n","        if any([model_dtype == torch.float16 for model_dtype in model_dtypes]):\n","          x = x.to('cuda', dtype=torch.float16)\n","        \n","        x1 = self.inc(x)\n","        x2 = self.down1(x1)\n","        x3 = self.down2(x2)\n","        x4 = self.down3(x3)\n","        x5 = self.down4(x4)\n","        x = self.up1(x5, x4)\n","        x = self.up2(x, x3)\n","        x = self.up3(x, x2)\n","        x = self.up4(x, x1)\n","        logits = self.outc(x)\n","        return logits\n","\n","    def use_checkpointing(self):\n","        self.inc = torch.utils.checkpoint(self.inc)\n","        self.down1 = torch.utils.checkpoint(self.down1)\n","        self.down2 = torch.utils.checkpoint(self.down2)\n","        self.down3 = torch.utils.checkpoint(self.down3)\n","        self.down4 = torch.utils.checkpoint(self.down4)\n","        self.up1 = torch.utils.checkpoint(self.up1)\n","        self.up2 = torch.utils.checkpoint(self.up2)\n","        self.up3 = torch.utils.checkpoint(self.up3)\n","        self.up4 = torch.utils.checkpoint(self.up4)\n","        self.outc = torch.utils.checkpoint(self.outc)"]},{"cell_type":"markdown","metadata":{"id":"P-QJ4yqUUn_5"},"source":["#### Limit training dataset to only one material for simplicity"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1709897822364,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"4uVBxUTGUn_5","outputId":"13e58941-e644-4681-8b6e-03488f144396"},"outputs":[],"source":["limit_dataset = False # set to True to limit the dataset to one material for faster training\n","\n","if limit_dataset:\n","    # limit the training dataset to only one material:\n","    train_loader.dataset.dataframe = train_loader.dataset.dataframe[train_loader.dataset.dataframe['material'] == 'Wood']\n","    print(\"Number of material in training set:\", len(train_loader.dataset.dataframe['material'].unique()))\n","    print(f\"Number of images in the training set: {len(train_loader.dataset)}\")\n","\n","    image_path = train_loader.dataset.dataframe.iloc[-1]['image_path']\n","    image_annotation_path = train_loader.dataset.dataframe.iloc[-1]['annotation_rgb_path']\n","\n","\n","    # from IPython.display import display, Image\n","    # display(Image(filename=image_path))\n","    # display(Image(filename=image_annotation_path))\n"]},{"cell_type":"markdown","metadata":{"id":"JgodW2l4Un_5"},"source":["### Loss Function:"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1709897822364,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"QjN_-hu4Un_6"},"outputs":[],"source":["\n","class DiceLoss(nn.Module):\n","    def __init__(self, smooth=1, class_weights=None):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","        self.class_weights = class_weights  # Optionally, provide class weights\n","\n","    def forward(self, prediction, target):\n","        \"\"\"\n","        Calculates the Dice coefficient per class and returns the average.\n","\n","        Args:\n","            prediction: Tensor of predictions (N, C, H, W) \n","            target: Tensor of ground truth labels (N, H, W), where each value\n","                    is an integer representing the class label.\n","        \"\"\"\n","\n","        # One-hot encode the target to simplify calculations\n","        target = torch.eye(prediction.size(1)).to(prediction.device)[target.squeeze(1)]  # (N, C, H, W)\n","        target = target.permute(0, 3, 1, 2).contiguous()  # (N, H, W, C) \n","\n","        # Calculate intersection and union for each class\n","        intersection = (prediction * target).sum(dim=(1, 2))\n","        union = prediction.sum(dim=(1, 2)) + target.sum(dim=(1, 2))\n","\n","        # Apply class weights (if provided)\n","        if self.class_weights is not None:\n","            intersection = intersection * self.class_weights.to(prediction.device)\n","            union = union * self.class_weights.to(prediction.device)\n","\n","        # Calculate Dice coefficient per class\n","        if torch.isnan(union).any():\n","            union[torch.isnan(union)] = 0.0\n","        if torch.isnan(intersection).any():\n","            intersection[torch.isnan(intersection)] = 0.0\n","        \n","        dominator = union + self.smooth\n","        nominator = 2.0 * intersection + self.smooth\n","        dice_coefficient = nominator / dominator\n","        \n","        # if dice_coefficient is nan, set it to 0\n","        if torch.isnan(dice_coefficient).any() or torch.isinf(dice_coefficient).any():\n","            dice_coefficient[:] = 0.0\n","\n","        # Calculate mean Dice loss over all classes\n","        dice_loss = 1 - dice_coefficient.mean()\n","\n","        return dice_loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# #' Shape of prediction: torch.Size([1, 17, 512, 512]), dtype: torch.float32\n","# #' Shape of target: torch.Size([1, 512, 512]), dtype: torch.int64\n","# # Sample tensors with adjusted shapes\n","# prediction = torch.rand(4, 17, 512, 512)  # 1 batch, 2 classes, 512x512 \n","# target = torch.randint(0, 17, (4, 512, 512))  # 1 batch, 512x512, class labels\n","# prediction = prediction.to(DEVICE)\n","# target = target.to(DEVICE)\n","\n","# print(f\"Shape of prediction: {prediction.shape}, dtype: {prediction.dtype}\")\n","# print(f\"Shape of target: {target.shape}, dtype: {target.dtype}\")\n","\n","# # --- CASE 1: Without class weights ---\n","# dice_loss_no_weights = DiceLoss()\n","# loss_no_weights = dice_loss_no_weights(prediction, target)\n","# print(\"Loss without weights:\", loss_no_weights.item())\n","\n","# # --- CASE 2: With class weights (ignore class 0) ---\n","# class_weights = torch.tensor([0.0, 1.0])\n","# class_weights = class_weights.view(1, 2, 1, 1)  # Expand dimensions\n","# print(\"Class weights:\", class_weights)\n","# print(f\"class weights shape: {class_weights.shape}\")\n","# dice_loss_with_weights = DiceLoss(class_weights=class_weights)\n","# loss_with_weights = dice_loss_with_weights(prediction, target)\n","# print(\"Loss with weights (ignoring class 0):\", loss_with_weights.item())\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_epoch(model, train_loader, loss_fn, optimiser):\n","    model.train()\n","    train_loss = []\n","    for batch in train_loader:\n","        images = batch['image'].to(DEVICE)\n","        targets = batch['annotation'].to(DEVICE)\n","        outputs = model(images)\n","        loss = loss_fn(outputs, targets)\n","        \n","        # Backward pass:\n","        optimiser.zero_grad()\n","        loss.backward()\n","        optimiser.step()\n","\n","        train_loss.append(loss.detach().cpu().numpy())\n","\n","    return np.mean(train_loss)\n","\n","def test_epoch(model, test_loader, loss_fn):\n","    model.eval()\n","    test_loss = []\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            images = batch['image'].to(DEVICE)\n","            targets = batch['annotation'].to(DEVICE)\n","\n","            outputs = model(images)\n","            loss = loss_fn(outputs, targets)\n","\n","            test_loss.append(loss.detach().cpu().numpy())\n","    return np.mean(test_loss)\n"]},{"cell_type":"markdown","metadata":{"id":"I3IRM7wIUn_6"},"source":["### Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"yrqSBunyUn_6"},"source":["#### Hyperparameters optimisation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":412,"status":"ok","timestamp":1709897822763,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"utM_BQ-QUn_6"},"outputs":[],"source":["\n","torch.manual_seed(0)\n","\n","num_classes = len(class_names) + 1  # 16 damage classes + 1 background class\n","\n","in_channels = train_loader.dataset[-1]['image'].shape[-3]\n","out_channels = num_classes\n","\n","# class_weights = None\n","class_weights = torch.ones(num_classes).to(DEVICE)\n","class_weights[0] = 0  # ignore the background class\n","class_weights = class_weights.float()\n","class_weights = class_weights.view(1, num_classes, 1, 1)  # Expand dimensions\n","# print(\"Class weights shape:\", class_weights.shape)\n","\n","hyperparameters = { \n","    \"lr\": 1e-3,\n","    \"momentum\": 0.2,\n","    \"smooth\": 1\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1709897822763,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"mQK8ZfnaUoAe","outputId":"835b23ac-0be2-4ba7-9a8f-9f919db8b0a9"},"outputs":[],"source":["optimise_parameters = True\n","\n","if optimise_parameters:\n","\n","    from ax.service.managed_loop import optimize\n","    from ax.plot.trace import optimization_trace_single_method\n","    from ax.utils.notebook.plotting import render, init_notebook_plotting\n","\n","    def train_evaluate(parameterisation):\n","        net = UNet(in_channels, out_channels).to(DEVICE)\n","        # net.enable_verbose(True)\n","        optimiser = optim.SGD(net.parameters(), lr=parameterisation.get('lr'), momentum=parameterisation.get('momentum'))\n","        # ignore the background class when calculating the loss:\n","        loss_fn = DiceLoss(class_weights=class_weights).to(DEVICE)\n","\n","        try:\n","            train_epoch(model=net, train_loader=train_loader, loss_fn=loss_fn, optimiser=optimiser)\n","        except Exception as e:\n","            print(f\"Error happened: {e}\")\n","\n","        prediction_dice_error = []\n","        for batch in val_loader:\n","            images = batch['image'].to(DEVICE)\n","            targets = batch['annotation'].to(DEVICE)\n","            with torch.no_grad():  # Ensure no gradients are calculated during validation\n","                outputs = net(images)\n","                loss = loss_fn(outputs, targets)\n","\n","            prediction_dice_error.append(loss.detach().cpu().numpy())\n","\n","        prediction_dice_error_mean = np.mean(prediction_dice_error)\n","        print(f\"Parameters: {parameterisation}, Prediction Correctness: {prediction_dice_error_mean}\")\n","\n","        if prediction_dice_error_mean < 1:\n","            torch.save(net.state_dict(), f'intermediate_models/unet_{prediction_dice_error_mean}_prec.pth')\n","\n","        return {\"Loss\": (prediction_dice_error_mean, 0.0)}\n","\n","    print(\"Optimising hyperparameters..\")\n","    parameters = [\n","        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-6, 1e-1], \"log_scale\": True},\n","        {\"name\": \"momentum\", \"type\": \"range\", \"bounds\": [0.0, 2.0]}\n","    ]\n","    total_trails = 1\n","    \n","    best_parameters, values, experiment, optimiser_model = optimize(\n","        parameters=parameters,\n","        evaluation_function=train_evaluate,\n","        objective_name=\"Loss\",\n","        minimize=True,\n","        total_trials=total_trails\n","    )\n","\n","    hyperparameters[\"lr\"] = round(best_parameters['lr'], 5)\n","    hyperparameters[\"momentum\"] = round(best_parameters['momentum'], 2)\n","\n","    print(\"\\nFinished optimising hyperparameters.\")\n","    print(\"Best parameters:\")\n","    [print(f\"\\t{key}: {value}\") for key, value in best_parameters.items()]\n","\n","    init_notebook_plotting(offline=True)\n","    best_objectives = np.array([[trial.objective_mean*100 for trial in experiment.trials.values()]])\n","    data = optimization_trace_single_method(best_objectives, title=\"Optimization trace\", ylabel=\"Similarity\")\n","    render(data)\n","\n","else:\n","    print(\"Using default hyperparameters..\")\n","    [print(f\"{key}: {value}\") for key, value in hyperparameters.items()]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1709898817630,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"MItNCJgJUoAf","outputId":"ab654226-8e08-40e7-e986-bc8e0e3d3531"},"outputs":[],"source":["\n","print(f\"Initialising model with {in_channels} input channels and {out_channels} output channels.\")\n","model = UNet(in_channels, out_channels, bilinear=False).to(DEVICE)\n","model = model.half() # half the precision\n","\n","optimiser = optim.SGD(model.parameters(), lr=hyperparameters['lr'], momentum=hyperparameters['momentum'])\n","\n","loss_fn = DiceLoss(smooth=hyperparameters['smooth'], class_weights=class_weights).to(DEVICE)\n","loss_fn = loss_fn.to(DEVICE)\n","# loss_fn = nn.MSELoss().to(DEVICE)\n","print(\"########## Finished initialising model and training parameters. ##########\")"]},{"cell_type":"markdown","metadata":{"id":"4GLsh85uUoAf"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":729,"status":"ok","timestamp":1709898819270,"user":{"displayName":"Jalal Sayed","userId":"17119264267530048208"},"user_tz":0},"id":"-EQbkgN1UoAf","outputId":"9032c690-c74e-410d-8d97-e24d04f4a8b3"},"outputs":[],"source":["test_model = False # set to True to test the model correctness before training\n","\n","if test_model:\n","    from torchsummary import summary\n","    # model.enable_verbose(True)\n","    image_size = train_loader.dataset[-1]['image'].shape[-1] # square images so only one dimension is needed\n","    summary(model, (in_channels, image_size, image_size))\n","    # model.enable_verbose(False)\n"]},{"cell_type":"markdown","metadata":{"id":"P4DgZAqmUoAf"},"source":["#### Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ant1oURUoAf","outputId":"2e414326-e944-4c19-faf1-1460a978ddb1"},"outputs":[],"source":["\n","train_model_again = True # set to True to train the model again.\n","\n","training_performance = {'train_loss': []}\n","validation_performance = {'val_loss': []}\n","\n","if train_model_again:\n","    num_epochs = 200\n","    for epoch in range(num_epochs):\n","        train_loss = train_epoch(model, train_loader, loss_fn, optimiser)\n","        val_loss = test_epoch(model, val_loader, loss_fn)\n","\n","        if (epoch < 10) or ((epoch + 1) % 5 == 0):\n","            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n","\n","        training_performance['train_loss'].append(train_loss)\n","        validation_performance['val_loss'].append(val_loss)\n","\n","    print(\"Training complete!\")\n","    # Plot the training and validation losses\n","    plt.figure(figsize=(20, 5))\n","    \n","    plt.plot(training_performance['train_loss'], label='Train Loss')\n","    plt.plot(validation_performance['val_loss'], label='Val Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.title('Training and Validation Losses')\n","\n","    plt.title('Training and Validation Accuracies')\n","    plt.legend()\n","\n","    # save plot:\n","    plt.savefig(f'img/training_results/losses_lr={hyperparameters[\"lr\"]}_momentum={hyperparameters[\"momentum\"]}.png')\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9RXbIYOmUoAf"},"outputs":[],"source":["save_model = True # set to True to save the model to github after training.\n","\n","# torch.save(model.state_dict(), 'unet.pth')\n","\n","if save_model:\n","    # Save model locally:\n","    torch.save(model.state_dict(), 'unet.pth')\n","    # Save model to github:\n","    !git add unet.pth\n","    !git commit -m \"Add trained UNet model\"\n","    !git push"]},{"cell_type":"markdown","metadata":{"id":"ZFLrakWrUoAf"},"source":["### Load model from github"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKbzmTK2UoAf"},"outputs":[],"source":["load_model = False # set to True to load the model from github after training.\n","\n","if load_model:\n","\n","    import requests\n","\n","    def download_model(url, model_path):\n","        r = requests.get(url, allow_redirects=True)\n","        if r.status_code == 200:\n","            # Override the model.pth file if it already exists:\n","            with open(model_path, 'wb') as f:\n","                f.write(r.content)\n","            print(f\"Model downloaded to {model_path}\")\n","            return model_path\n","        else:\n","            print(f\"Failed to download model. Status code: {r.status_code}\")\n","            return None\n","\n","    #! update id:\n","    model_url = 'https://raw.githubusercontent.com/JalalSayed1/Image-Damage-Classification/3baa846923307ae573f4fbecd7c8f00fc269fad4/unet.pth'\n","\n","    model_path = download_model(model_url, 'unet.pth')\n","    if model_path:\n","        print(\"Model downloaded successfully!\")\n","    else:\n","        print(\"Failed to download model.\")\n"]},{"cell_type":"markdown","metadata":{"id":"HdD7ATuHUoAf"},"source":["### Test Model performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mg0r0Ml9UoAf"},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def map_class_ids_to_rgb(input_tensor, id_to_rgb, id2label):\n","    # Assuming input_tensor is of shape [batch, C, H, W]\n","    batch_size, C, H, W = input_tensor.shape\n","\n","    # Initialize the RGB image tensor with shape [batch, 3, H, W]\n","    rgb_image = torch.zeros((batch_size, 3, H, W), dtype=torch.uint8)\n","\n","    # Iterate over each image in the batch\n","    for batch_idx in range(batch_size):\n","        # Extract the class ID tensor for the current image\n","        # If input_tensor is actually [1, 3, H, W] representing class IDs, you need to ensure it's a single-channel image.\n","        # Ensure input_tensor is of shape [batch, 1, H, W] if each pixel represents a class ID.\n","        class_ids_2d = input_tensor[batch_idx, 0, :, :]  # Assuming class IDs are in a single channel\n","\n","        classes_freq = {} #!\n","\n","        # Iterate over each class ID as before\n","        for class_id, rgb in id_to_rgb.items():\n","            if class_id == 17:\n","                class_id = 255\n","\n","            class_mask = (class_ids_2d == class_id)\n","            classes_freq[class_id] = class_mask.sum() #!\n","\n","            for channel, color_value in enumerate(rgb):\n","                # Correctly use the class_mask for indexing\n","                # Here, we need to expand class_mask to match rgb_image dimensions for broadcasting\n","                rgb_image[batch_idx, channel, class_mask] = color_value\n","\n","        for k, v in sorted(classes_freq.items(), key=lambda item: item[1], reverse=True):\n","            print(f\"Class ID: {k:<5} \\t Damage {id2label[k]:<10} \\t Num of pixels: {v:<10}\") #!\n","        print()\n","\n","    return rgb_image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b53yf5OcUoAg"},"outputs":[],"source":["from torchvision.transforms.functional import to_pil_image\n","\n","model.load_state_dict(torch.load('unet.pth', map_location=DEVICE))\n","model.eval()\n","print(\"Model loaded successfully!\")\n","\n","\n","N = min(5, len(test_loader.dataset))\n","print(f\"Number of examples to visualize: {N}\\n\")\n","\n","# print(f\"example image shape: {test_loader.dataset[-1]['image'].shape}\")\n","test_batches = [test_loader.dataset[i] for i in range(N)]\n","\n","class_weights = torch.ones(num_classes).to(DEVICE)\n","class_weights[0] = 0  # ignore the background class\n","class_weights = class_weights.float()\n","class_weights = class_weights.view(1, num_classes, 1, 1)  # Expand dimensions\n","loss_fn = DiceLoss(class_weights=class_weights).to(DEVICE)\n","\n","# empty the predictions folder to save the new predictions:\n","predictions_folder = 'img/predictions/test_results'\n","for file in os.listdir(predictions_folder):\n","    file_path = os.path.join(predictions_folder, file)\n","    if os.path.isfile(file_path):\n","        os.unlink(file_path) # delete the file\n","\n","for index, batch in enumerate(test_batches):\n","    test_image = batch['image'].unsqueeze(0).to(DEVICE)\n","    test_annotation = (batch['annotation'].squeeze()).to(DEVICE)\n","    test_annotation_rgb = batch['annotation_rgb'].squeeze()\n","    predicted_annotation = model(test_image)\n","\n","    # print(f\"Shapes:\")\n","    # print(f\"\\ttest_image: {test_image.shape}\")\n","    # print(f\"\\ttest_annotation: {test_annotation.shape}\")\n","    # print(f\"\\ttest_annotation_rgb: {test_annotation_rgb.shape}\")\n","    # print(f\"\\tpredicted_annotation: {predicted_annotation.shape}\")\n","\n","    predicted_annotation = torch.argmax(predicted_annotation, dim=0).unsqueeze(0)\n","    # predicted_annotation_rgb = indices_to_rgb(predicted_annotation, id_to_rgb)\n","\n","    fig, axes = plt.subplots(1, 5, figsize=(20, 5))\n","\n","    similarity = round(1 - loss_fn(predicted_annotation, test_annotation.unsqueeze(0)).item(), 2)\n","    plt.suptitle(f\"{index}. Annotation Similarity: {similarity*100}%\", fontsize=24, ha='center', va='top', y=1.1)\n","\n","    titles = ['Image', 'True Annotation', 'True Overlay', 'Predicted Anno.', 'Predicted Overlay']\n","    for ax, title in zip(axes, titles):\n","        ax.set_title(title, fontsize=24)\n","\n","    test_image = denormalize(test_image.squeeze().cpu().numpy().transpose((1, 2, 0)), mean, std)  # C, H, W -> H, W, C\n","    \n","    test_annotation_rgb = test_annotation_rgb.numpy().transpose((1, 2, 0))  # C, H, W -> H, W, C\n","\n","    predicted_annotation_rgb = map_class_ids_to_rgb(predicted_annotation, id_to_rgb, id2label).squeeze(0)\n","    # C, H, W -> H, W, C\n","    predicted_annotation_rgb = predicted_annotation_rgb.cpu().numpy().transpose((1, 2, 0))\n","    \n","    plt.imsave(os.path.join(predictions_folder, f\"{index}.png\"), predicted_annotation_rgb)\n","\n","    axes[0].imshow(test_image)\n","    axes[0].axis('off')\n","\n","    axes[1].imshow(test_annotation_rgb)\n","    axes[1].axis('off')\n","\n","    alpha_channel = np.all(test_annotation_rgb == [0, 0, 0], axis=-1)\n","    test_annotation_rgba = np.dstack((test_annotation_rgb, np.where(alpha_channel, 0, 1)))\n","    axes[2].imshow(test_image)\n","    axes[2].imshow(test_annotation_rgba)\n","    axes[2].axis('off')\n","\n","    axes[3].imshow(predicted_annotation_rgb)\n","    axes[3].axis('off')\n","\n","    predicted_annotation_rgba = np.dstack((predicted_annotation_rgb, np.where(alpha_channel, 0, 1)))\n","    axes[4].imshow(test_image)\n","    axes[4].imshow(predicted_annotation_rgba)\n","    axes[4].axis('off')\n","\n","'''\n","{'Material loss': ('#1CE6FF', (0, 0, 0), 'Black'),\n"," 'Peel': ('#FF34FF', (28, 230, 255), 'Cyan / Aqua'),\n"," 'Dust': ('#FF4A46', (255, 52, 255), 'Magenta / Fuchsia'),\n"," 'Scratch': ('#008941', (255, 74, 70), 'Red'),\n"," 'Hair': ('#006FA6', (0, 137, 65), 'Teal'),\n"," 'Dirt': ('#A30059', (0, 111, 166), 'Teal'),\n"," 'Fold': ('#FFA500', (163, 0, 89), 'Purple'),\n"," 'Writing': ('#7A4900', (255, 165, 0), 'Yellow'),\n"," 'Cracks': ('#0000A6', (122, 73, 0), 'Olive'),\n"," 'Staining': ('#63FFAC', (0, 0, 166), 'Navy'),\n"," 'Stamp': ('#004D43', (99, 255, 172), 'Silver'),\n"," 'Sticker': ('#8FB0FF', (0, 77, 67), 'Teal'),\n"," 'Puncture': ('#997D87', (143, 176, 255), 'Silver'),\n"," 'Background': ('#5A0007', (153, 125, 135), 'Gray'),\n"," 'Burn marks': ('#809693', (128, 150, 147), 'Gray'),\n"," 'Lightleak': ('#f6ff1b', (246, 255, 27), 'Yellow')}\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfVLT20QUoAg"},"outputs":[],"source":["# !pip freeze > requirements.txt"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}
