{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JalalSayed1/DL_CW/blob/master/DL_CW_damages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bun5mMvaeOE3"
      },
      "source": [
        "## GUID: 2571964S\n",
        "\n",
        "## to describe and motivate your design choices: architecture, pre-processing, training regime\n",
        "\n",
        "## to analyse, describe and comment on your results\n",
        "\n",
        "## to provide some discussion on what you think are the limitations of your solution and what could be future work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Can discuss:\n",
        "Choice of Architecture: Explain why you chose a specific architecture over others. For instance, U-Net might be chosen for its efficiency and small dataset effectiveness, while DeepLab could be chosen for its state-of-the-art performance on segmentation tasks.\n",
        "\n",
        "Preprocessing: Describe any preprocessing steps you take, such as resizing images, normalising pixel values, data augmentation, etc.\n",
        "\n",
        "Postprocessing: If you apply any postprocessing to the segmentation maps, such as CRFs to sharpen the boundaries, explain why and how this improves the results.\n",
        "\n",
        "Loss Functions: Discuss the choice of loss function, which in the case of segmentation could be cross-entropy, dice coefficient, or a combination of several loss functions.\n",
        "\n",
        "Metrics: Describe the metrics you'll use to evaluate the performance of your model, such as pixel accuracy, mean Intersection over Union (IoU), etc.\n",
        "\n",
        "Training Strategy: Detail the training process, including the choice of optimiser, learning rate, batch size, and any other hyperparameters.\n",
        "\n",
        "Results and Analysis: Present the results and provide an analysis of what worked and what didnâ€™t. Discuss any challenges you faced and how you addressed them.\n",
        "\n",
        "Visualisation: Explain the importance of visualisation in understanding the performance of your model. For example, overlay images help to see where the model is performing well and where it is making mistakes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2H6z8b9eYdY"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE4PJVsod6A3"
      },
      "source": [
        "# Damages - Deep Learning Coursework 2024\n",
        "\n",
        "The aim of this coursework will be for you to design, implement and test a deep learning architecture to detect and identify damage in images. Digitization allows to make historical pictures and art much more widely available to the public. Many such pictures have suffered some form of damage due to time, storage conditions and the fragility of the original medium. For example, the image below (A) shows an example of a digitized parchment that has suffered significant damage over time.\n",
        "\n",
        "**The aim of this project is for you to design, implement and evaluate a deep learning model to detect and identify damage present in images.**\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<td>\n",
        "<div>\n",
        "<img src=\"damage_data/image_path/cljmrkz5n341f07clcujw105j.png\" width=\"500\"/>\n",
        "</div>\n",
        "</td>\n",
        "<td>\n",
        "<div>\n",
        "<img src=\"damage_data/annotation_rgb_path/cljmrkz5n341f07clcujw105j.png\" width=\"500\"/>\n",
        "</div>\n",
        "</td>\n",
        "</tr>\n",
        "<td><center>(A) Image</center></td><td><center>(B) damage labels</center></td>\n",
        "</table>\n",
        "*(Note that the images will only show once you have downloaded the dataset)*\n",
        "\n",
        "\n",
        "The image labels in this figure (B) identifies a smatter of peeling paint, a large stained area in the bottom left and a missing part on the top left. Each colour in those images corresponds to a different category of damage, including `fold`, `writing` or `burn marks`. You are provided with a dataset of a variety of damaged images, from Parchment to ceramic or wood painting, and detailed annotations of a range of damages.\n",
        "\n",
        "You are free to use any architecture you prefer, from what we have seen in class. You can decide to use unsupervised pre-training of only supervised end-to-end training - the approach you choose is your choice.\n",
        "\n",
        "### Hand-in date: Friday 15th of March before 4:30pm (on Moodle)\n",
        "\n",
        "### Steps & Hints\n",
        "* First, look at the data. What are the different type of images (content), what type of material, what type of damage? How different are they? What type of transformations for your data augmentation do you think would be acceptable here?.\n",
        "* Second, check the provided helper functions for loading the data and separate into training and test set and cross-validation.\n",
        "* Design a network for the task. What output? What layers? How many? Do you want to use an Autoencoder for unsupervised pre-training?\n",
        "* Choose a loss function for your network\n",
        "* Select optimiser and training parameters (batch size, learning rate)\n",
        "* Optimise your model, and tune hyperparameters (especially learning rate, momentum etc)\n",
        "* Analyse the results on the test data. How to measure success? Which classes are recognised well, which are not? Is there confusion between some classes? Look at failure cases.\n",
        "* If time allows, go back to drawing board and try a more complex, or better, model.\n",
        "* Explain your thought process, justify your choices and discuss the results!\n",
        "\n",
        "### Submission\n",
        "* submit ONE zip file on Moodle containing:\n",
        "  * **your notebook**: use `File -> download .ipynb` to download the notebook file locally from colab.\n",
        "  * **a PDF file** of your notebook's output as you see it: use `File -> print` to generate a PDF.\n",
        "* your notebook must clearly contains separate cells for:\n",
        "  * setting up your model and data loader\n",
        "  * training your model from data\n",
        "  * loading your pretrained model from github/gitlab/any other online storage you like!\n",
        "  * testing your model on test data.\n",
        "* The training cells must be disabled by a flag, such that when running *run all* on your notebook it does\n",
        "  * load the data\n",
        "  * load your model\n",
        "  * apply the model to the test data\n",
        "  * analyse and display the results and accuracy\n",
        "* In addition provide markup cell:\n",
        "  * containing your student number at the top\n",
        "  * to describe and motivate your design choices: architecture, pre-processing, training regime\n",
        "  * to analyse, describe and comment on your results\n",
        "  * to provide some discussion on what you think are the limitations of your solution and what could be future work\n",
        "\n",
        "* **Note that you must put your trained model online so that your code can download it.**\n",
        "\n",
        "\n",
        "### Assessment criteria\n",
        "* In order to get a pass mark, you will need to demonstrate that you have designed and trained a deep NN to solve the problem, using sensible approach and reasonable efforts to tune hyper-parameters. You have analysed the results. It is NOT necessary to have any level of accuracy (a network that predicts poorly will always yield a pass mark if it is designed, tuned and analysed sensibly).\n",
        "* In order to get a good mark, you will show good understanding of the approach and provide a working solution.\n",
        "* in order to get a high mark, you will demonstrate a working approach of gradual improvement between different versions of your solution.\n",
        "* bonus marks for attempting something original if well motivated - even if it does not yield increased performance.\n",
        "* bonus marks for getting high performance, and some more points are to grab for getting the best performance in the class.\n",
        "\n",
        "### Notes\n",
        "* You are provided code to isolate the test set and cross validation, make sure to keep the separation clean to ensure proper setting of all hyperparameters.\n",
        "* I recommend to start with small models that can be easier to train to set a baseline performance before attempting more complex one.\n",
        "* Be mindful of the time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0KpHbRKqhn_",
        "outputId": "1bb5d2e3-4fdc-43c6-f11c-f62da7051db0"
      },
      "outputs": [],
      "source": [
        "using_colab = False\n",
        "if using_colab:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQa07mdpd6A6"
      },
      "source": [
        "## Housekeeping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtWZa-JPesz7",
        "outputId": "f7836cca-46c7-47be-b97d-cb0306c74396"
      },
      "outputs": [],
      "source": [
        "!pip install gdown pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK_rOEmQuwV3"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import PIL\n",
        "PIL.Image.MAX_IMAGE_PIXELS = 243748701\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import shutil\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CU2Ky6meo0kt"
      },
      "source": [
        "# Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLMiDRyUd6A8"
      },
      "source": [
        "We then load the metadata in a dataframe for convenience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nM_cNHrgsjY",
        "outputId": "2231cb84-c2d3-4000-dbaa-9b54393ad0f2"
      },
      "outputs": [],
      "source": [
        "if using_colab:\n",
        "    !pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_T3a3WHiL8P",
        "outputId": "3b0b10c5-c88c-4f86-965c-5ed44451b95b"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"damage_data\"):\n",
        "    !gdown 1v8aUId0-tTW3ln3O2BE4XajQeCToOEiS -O damages.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S7fMugEd6A8",
        "outputId": "b5b7c3ab-a532-426f-df1f-9faa7fc073a0"
      },
      "outputs": [],
      "source": [
        "# set  that to wherever you want to store the data (eg, your Google Drive), choose a persistent location!\n",
        "root_dir = '.'\n",
        "data_dir = os.path.join(root_dir, \"damage_data\")\n",
        "csv_path = os.path.join(data_dir, 'metadata.csv')\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "except:  # if the dataset has not been downloaded yet, do it.\n",
        "    zip_path = os.path.join(root_dir, 'damages.zip')\n",
        "    gdown.download(id='1v8aUId0-tTW3ln3O2BE4XajQeCToOEiS', output=zip_path)\n",
        "    shutil.unpack_archive(zip_path, root_dir)\n",
        "    df = pd.read_csv(csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18VmATnob3G_"
      },
      "source": [
        "This dataframe has the paths of where the dataset images and annotation labels are stored, plus classification labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "obKQZoPbd6A8",
        "outputId": "7a47dd36-9897-420a-8dec-ca6449b1ee77"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DMf6xlKd6A9"
      },
      "source": [
        "The images in the dataset are categorised in terms of the type of `material`, meaning what was the original picture on, eg, Parchment, Glass or Textile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyWU2V3Dd6A9",
        "outputId": "08ef4982-f82e-40dc-f117-5b0c91c6c537"
      },
      "outputs": [],
      "source": [
        "df['material'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ12Mf2Ud6A9"
      },
      "source": [
        "Moreover, images are also categorised in terms on the `content` of the image, meaning what is depicted: eg, Line art, geometric patterns, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1SzCR0qd6A9",
        "outputId": "d3add62a-611d-4042-cb1f-843c32ef713f"
      },
      "outputs": [],
      "source": [
        "df['content'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8rH3KykdMNc"
      },
      "source": [
        "## Labels\n",
        "Segmentation labels are saved as a PNG image, where each number from 1 to 15 corresponds to a damage class like Peel, Scratch etc; the Background class is set to 255, and the Clean class (no damage) is set to 0. We also provide code to convert these annotation values to RGB colours for nicer visualisation, but for training you should use the original annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Td8pW0M6WLZO"
      },
      "outputs": [],
      "source": [
        "name_color_mapping = {\n",
        "    \"Material loss\": \"#1CE6FF\",\n",
        "    \"Peel\": \"#FF34FF\",\n",
        "    \"Dust\": \"#FF4A46\",\n",
        "    \"Scratch\": \"#008941\",\n",
        "    \"Hair\": \"#006FA6\",\n",
        "    \"Dirt\": \"#A30059\",\n",
        "    \"Fold\": \"#FFA500\",\n",
        "    \"Writing\": \"#7A4900\",\n",
        "    \"Cracks\": \"#0000A6\",\n",
        "    \"Staining\": \"#63FFAC\",\n",
        "    \"Stamp\": \"#004D43\",\n",
        "    \"Sticker\": \"#8FB0FF\",\n",
        "    \"Puncture\": \"#997D87\",\n",
        "    \"Background\": \"#5A0007\",\n",
        "    \"Burn marks\": \"#809693\",\n",
        "    \"Lightleak\": \"#f6ff1b\",\n",
        "}\n",
        "\n",
        "class_names = [ 'Material loss', 'Peel', 'Dust', 'Scratch',\n",
        "                'Hair', 'Dirt', 'Fold', 'Writing', 'Cracks', 'Staining', 'Stamp',\n",
        "                'Sticker', 'Puncture', 'Burn marks', 'Lightleak', 'Background']\n",
        "\n",
        "class_to_id = {class_name: idx+1 for idx, class_name in enumerate(class_names)}\n",
        "class_to_id['Background'] = 255  # Set the Background ID to 255\n",
        "\n",
        "def hex_to_rgb(hex_color: str) -> tuple:\n",
        "    hex_color = hex_color.lstrip('#')\n",
        "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "\n",
        "id_to_rgb = {class_to_id[class_name]: hex_to_rgb(color) for class_name, color in name_color_mapping.items()}\n",
        "id_to_rgb[0] = (0,0,0)\n",
        "\n",
        "# Create id2label mapping: ID to class name\n",
        "id2label = {idx: class_name for class_name, idx in class_to_id.items()}\n",
        "\n",
        "# Create label2id mapping: class name to ID, which is the same as class_to_id\n",
        "label2id = class_to_id\n",
        "\n",
        "# Non-damaged pixels\n",
        "id2label[0] = 'Clean'\n",
        "label2id['Clean'] = 0\n",
        "\n",
        "print(len(id_to_rgb))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "SoOFtDuud6A-",
        "outputId": "50b29c15-7819-452e-c043-24af292a135a"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "legend='#### Colour labels for each damage type\\n'\n",
        "for damage in class_names:\n",
        "    legend += '- <span style=\"color: {color}\">{damage}</span>.\\n'.format(color=name_color_mapping[damage], damage=damage)\n",
        "display(Markdown(legend))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oantk2PzcEFb"
      },
      "source": [
        "## Create dataset splits\n",
        "\n",
        "Here is an example of how to split the dataset for Leave-one-out cross validation (LOOCV) based on material."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay2LWSuLiDom"
      },
      "outputs": [],
      "source": [
        "def create_leave_one_out_splits(df, criterion='material'):\n",
        "\n",
        "    grouped = df.groupby(criterion)\n",
        "    content_splits = {name: group for name, group in grouped}\n",
        "    unique_val = df[criterion].unique()\n",
        "\n",
        "    # Initialize a dictionary to hold the train and validation sets for each LOOCV iteration\n",
        "    loocv_splits = {}\n",
        "\n",
        "    for value in unique_val:\n",
        "        # Create the validation set\n",
        "        val_set = content_splits[value]\n",
        "\n",
        "        # Create the training set\n",
        "        train_set = pd.concat([content_splits[c] for c in unique_val if c != value])\n",
        "\n",
        "        # Add these to the loocv_splits dictionary\n",
        "        loocv_splits[value] = {'train_set': train_set, 'val_set': val_set}\n",
        "\n",
        "    return loocv_splits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y5NruT7d6A-"
      },
      "source": [
        "For this coursework, we will want to assess the generalisation of the method, so for that we will keep one type of material (`Canvas`) as test set, and only train on the remaining ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNf29AtXd6A-",
        "outputId": "268c30ae-039b-4b54-b021-20db1caabcbf"
      },
      "outputs": [],
      "source": [
        "# split the dataset according to material type\n",
        "full_splits = create_leave_one_out_splits(df, 'material')\n",
        "\n",
        "# use Canvas as test set\n",
        "test_set = full_splits['Canvas']['val_set']\n",
        "\n",
        "# use the rest as training set\n",
        "train_set = full_splits['Canvas']['train_set']\n",
        "\n",
        "# prepare a leave-one-out cross validation for the training set\n",
        "loocv_splits = create_leave_one_out_splits(train_set, 'material')\n",
        "\n",
        "# identify the different type of image content\n",
        "unique_material = train_set['material'].unique()\n",
        "\n",
        "print(\"Training set materials:\", unique_material)\n",
        "print(\"Test set material:\", test_set['material'].unique())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJRkv1lXcNFq"
      },
      "source": [
        "To help you, here are some helper functions to help crop and process images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9bJdsQGd6A_"
      },
      "outputs": [],
      "source": [
        "def random_square_crop_params(image, target_size):\n",
        "    width, height = image.size\n",
        "    min_edge = min(width, height)\n",
        "\n",
        "    # Conditionally set the range for random crop size\n",
        "    lower_bound = min(min_edge, target_size)\n",
        "    upper_bound = max(min_edge, target_size)\n",
        "\n",
        "    # Generate crop_size\n",
        "    crop_size = random.randint(lower_bound, upper_bound)\n",
        "\n",
        "    # Check and adjust if crop_size is larger than any dimension of the image\n",
        "    if crop_size > width or crop_size > height:\n",
        "        crop_size = min(width, height)\n",
        "\n",
        "    # Generate random coordinates for the top-left corner of the crop\n",
        "    x = random.randint(0, width - crop_size)\n",
        "    y = random.randint(0, height - crop_size)\n",
        "\n",
        "    return (x, y, x + crop_size, y + crop_size)\n",
        "\n",
        "def apply_crop_and_resize(image, coords, target_size):\n",
        "    image_crop = image.crop(coords)\n",
        "    image_crop = image_crop.resize((target_size, target_size), Image.NEAREST)\n",
        "    return image_crop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOlkN-lVd6A_"
      },
      "source": [
        "We also provide a simple class for holding the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBjlhUmnd6A_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataframe, target_size, is_train=True):\n",
        "        self.dataframe = dataframe\n",
        "        self.target_size = target_size\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "\n",
        "        # Define the normalization transform\n",
        "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                              std=[0.229, 0.224, 0.225])\n",
        "\n",
        "    def __len__(self):\n",
        "            return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        image = Image.open(row['image_path']).convert('RGB')\n",
        "        annotation = Image.open(row['annotation_path']).convert('L')\n",
        "        annotation_rgb = Image.open(row['annotation_rgb_path']).convert('RGB')\n",
        "        id = row['id']\n",
        "        material = row['material']\n",
        "        content = row['content']\n",
        "\n",
        "        if self.is_train:\n",
        "            # Generate random square cropping coordinates\n",
        "            crop_coords = random_square_crop_params(image, self.target_size)\n",
        "\n",
        "            # Apply the same cropping and resizing to all\n",
        "            image = apply_crop_and_resize(image, crop_coords, self.target_size)\n",
        "            annotation = apply_crop_and_resize(annotation, crop_coords, self.target_size)\n",
        "            annotation_rgb = apply_crop_and_resize(annotation_rgb, crop_coords, self.target_size)\n",
        "        else:  # Validation\n",
        "            # Instead of cropping, downsize the images so that the longest edge is 1024 or less\n",
        "            # max_edge = max(image.size)\n",
        "            # if max_edge > 1024:\n",
        "            #     downsample_ratio = 1024 / max_edge\n",
        "            #     new_size = tuple([int(dim * downsample_ratio) for dim in image.size])\n",
        "\n",
        "            #     image = image.resize(new_size, Image.BILINEAR)\n",
        "            #     annotation = annotation.resize(new_size, Image.NEAREST)\n",
        "            #     annotation_rgb = annotation_rgb.resize(new_size, Image.BILINEAR)\n",
        "\n",
        "            # Generate random square cropping coordinates\n",
        "            crop_coords = random_square_crop_params(image, self.target_size)\n",
        "\n",
        "            # Apply the same cropping and resizing to all\n",
        "            image = apply_crop_and_resize(image, crop_coords, self.target_size)\n",
        "            annotation = apply_crop_and_resize(annotation, crop_coords, self.target_size)\n",
        "            annotation_rgb = apply_crop_and_resize(annotation_rgb, crop_coords, self.target_size)\n",
        "\n",
        "        # Convert PIL images to PyTorch tensors\n",
        "        image = self.to_tensor(image)\n",
        "        annotation = torch.tensor(np.array(annotation), dtype=torch.long)\n",
        "        annotation_rgb = self.to_tensor(annotation_rgb)\n",
        "\n",
        "        # Normalize the image\n",
        "        image = self.normalize(image)\n",
        "\n",
        "        # Change all values in annotation that are 255 to 16\n",
        "        #! why?\n",
        "        annotation[annotation == 255] = 16\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'annotation': annotation,\n",
        "            'annotation_rgb': annotation_rgb,\n",
        "            'id': id,\n",
        "            'material': material,\n",
        "            'content': content\n",
        "        }\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvI-9NaxcljF"
      },
      "source": [
        "Here we create a DataModule which encapsulates our training and validation DataLoaders; you can also do this manually by only using the Pytorch DataLoader class, lines 24 and 27."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPuGFmPGuIFk"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CustomDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, loocv_splits, current_material, target_size, batch_size=32, num_workers=4):\n",
        "        super().__init__()\n",
        "        self.loocv_splits = loocv_splits\n",
        "        self.current_material = current_material\n",
        "        self.target_size = target_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def prepare_data(self):\n",
        "        pass\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        # Load current train and validation set based on LOOCV iteration\n",
        "        train_df = self.loocv_splits[self.current_material]['train_set']\n",
        "        val_df = self.loocv_splits[self.current_material]['val_set'].sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "        self.train_dataset = CustomDataset(dataframe=train_df, target_size=self.target_size, is_train=True)\n",
        "        self.val_dataset = CustomDataset(dataframe=val_df, target_size=self.target_size, is_train=False) \n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=1, shuffle=False, num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR1Qgc4Sd6A_"
      },
      "source": [
        "The following will create a data module for validating on the first content in the list (`Parchment`) and training on all the other types of material (you will want to do that for each fold)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEIvJAAuwKBb"
      },
      "outputs": [],
      "source": [
        "num_workers = 4 if using_colab else 0\n",
        "data_module = CustomDataModule(loocv_splits=loocv_splits,\n",
        "                               current_material=unique_material[0],\n",
        "                               target_size=512,\n",
        "                               batch_size=4,\n",
        "                               num_workers=num_workers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkEbiloVd6BA"
      },
      "source": [
        "Finally, we can get the train and validation data loaders from the data module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2nVOsBIwZPq"
      },
      "outputs": [],
      "source": [
        "data_module.setup()\n",
        "train_loader = data_module.train_dataloader()\n",
        "val_loader = data_module.val_dataloader()\n",
        "\n",
        "print(\"Number of training batches:\", len(train_loader))\n",
        "print(\"Number of training samples:\", len(train_loader.dataset))\n",
        "# val dataset is set to have batch size of 1:\n",
        "print(\"Number of validation batches:\", len(val_loader))\n",
        "print(\"Number of validation samples:\", len(val_loader.dataset))\n",
        "print(\"image size:\", train_loader.dataset[-1]['image'].shape)\n",
        "print(\"annotation size:\", train_loader.dataset[-1]['annotation'].shape)\n",
        "print(\"number of material in training set:\", len(train_loader.dataset.dataframe['material'].unique()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MevlLaqfc-XA"
      },
      "source": [
        "# Dataset visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xShAlBKvc32-"
      },
      "source": [
        "We need to denormalise the images so we can display them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHT-CU9tyblk"
      },
      "outputs": [],
      "source": [
        "# Mean and std used for normalization\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "\n",
        "def denormalize(image, mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]):\n",
        "    img_cpy = image.copy()\n",
        "    for i in range(3):\n",
        "        img_cpy[..., i] = img_cpy[..., i] * std[i] + mean[i]\n",
        "    return img_cpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw2zLzOJdBfb"
      },
      "source": [
        "## Visualise training samples\n",
        "Random square crops of the images and correspoding RGB annotations on their own and overlaid onto the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zDlIgX7ZwdEr",
        "outputId": "cc66123c-0e08-48cc-98de-9547bec86bd2"
      },
      "outputs": [],
      "source": [
        "num_to_visualise = len(train_loader.dataset) # all images\n",
        "print(\"Number of image that can be visualized:\", num_to_visualise)\n",
        "\n",
        "example_batch = next(iter(train_loader))\n",
        "print(\"Shape of the image batch:\", example_batch['image'].shape)\n",
        "\n",
        "example_images = example_batch['image']\n",
        "example_annotations = example_batch['annotation']\n",
        "example_annotation_rgbs = example_batch['annotation_rgb']\n",
        "\n",
        "# Number of examples to visualize\n",
        "# N = min(4, len(example_images))\n",
        "N = min(num_to_visualise, len(example_images))\n",
        "print(\"Number of examples to visualize:\", N)\n",
        "\n",
        "fig, axes = plt.subplots(N, 3, figsize=(15, 5 * N))\n",
        "\n",
        "for ax, col in zip(axes[0], ['Image', 'Annotation', 'Overlay']):\n",
        "    ax.set_title(col, fontsize=24)\n",
        "\n",
        "for i in range(N):\n",
        "    example_image = denormalize(example_images[i].numpy().transpose((1, 2, 0)), mean, std)  # C, H, W -> H, W, C\n",
        "    example_annotation = Image.fromarray(np.uint8(example_annotations[i].numpy()), 'L')\n",
        "    example_annotation_rgb = example_annotation_rgbs[i].numpy().transpose((1, 2, 0))  # C, H, W -> H, W, C\n",
        "\n",
        "    # Create an alpha (transparency) channel where black pixels in annotation_rgb are fully transparent\n",
        "    alpha_channel = np.all(example_annotation_rgb == [0, 0, 0], axis=-1)\n",
        "    example_annotation_rgba = np.dstack((example_annotation_rgb, np.where(alpha_channel, 0, 1)))\n",
        "\n",
        "    axes[i, 0].imshow(example_image)\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    #axes[i, 1].imshow(example_annotation, cmap='gray', vmin=0, vmax=255)\n",
        "    axes[i, 1].imshow(example_annotation_rgb)\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "    axes[i, 2].imshow(example_image)\n",
        "    axes[i, 2].imshow(example_annotation_rgba)\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t71C8ZeXdyWS"
      },
      "source": [
        "Visualising the validation set, which loads the left-out class as whole images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "anaTYaT6RNvH",
        "outputId": "b9fb01d8-c1e0-4415-de71-1afbaa0dc788"
      },
      "outputs": [],
      "source": [
        "val_iter = iter(val_loader)\n",
        "example_batches = [next(val_iter) for _ in range(N)]\n",
        "\n",
        "# Initialize empty lists to collect different parts of each batch\n",
        "example_images = []\n",
        "example_annotations = []\n",
        "example_annotation_rgbs = []\n",
        "example_materials = []\n",
        "example_contents = []\n",
        "\n",
        "# Populate the lists with the data from the 4 batches\n",
        "for batch in example_batches:\n",
        "    example_images.append(batch['image'].squeeze())\n",
        "    example_annotations.append(batch['annotation'].squeeze())\n",
        "    example_annotation_rgbs.append(batch['annotation_rgb'].squeeze())\n",
        "    example_materials.append(batch['material'][0])\n",
        "    example_contents.append(batch['content'][0])\n",
        "    \n",
        "    print(\"batch image shape:\", batch['image'].shape)\n",
        "    print(\"batch annotation shape:\", batch['annotation'].shape)\n",
        "    print(\"Shape of the image batch:\", example_images[0].shape)\n",
        "    print(\"Shape of the annotation batch:\", example_annotations[0].shape)\n",
        "\n",
        "# Number of examples to visualize\n",
        "# N = min(4, len(example_images))\n",
        "N = min(num_to_visualise, len(example_images))\n",
        "\n",
        "fig, axes = plt.subplots(N, 3, figsize=(15, 5 * N))\n",
        "\n",
        "for ax, col in zip(axes[0], ['Image', 'Annotation', 'Overlay']):\n",
        "    ax.set_title(col, fontsize=24)\n",
        "\n",
        "for i in range(N):\n",
        "    example_image = denormalize(example_images[i].numpy().transpose((1, 2, 0)), mean, std)  # C, H, W -> H, W, C\n",
        "    example_annotation = example_annotations[i].numpy()\n",
        "    example_annotation_rgb = example_annotation_rgbs[i].numpy().transpose((1, 2, 0))  # C, H, W -> H, W, C\n",
        "    example_material = example_materials[i]\n",
        "    example_content = example_contents[i]\n",
        "    # Create an alpha (transparency) channel where black pixels in annotation_rgb are fully transparent\n",
        "    alpha_channel = np.all(example_annotation_rgb == [0, 0, 0], axis=-1)\n",
        "    example_annotation_rgba = np.dstack((example_annotation_rgb, np.where(alpha_channel, 0, 1)))\n",
        "    axes[i, 0].imshow(example_image)\n",
        "    axes[i, 0].axis('off')\n",
        "\n",
        "    axes[i, 1].imshow(example_annotation_rgb)\n",
        "    axes[i, 1].axis('off')\n",
        "\n",
        "    axes[i, 2].imshow(example_image)\n",
        "    axes[i, 2].imshow(example_annotation_rgba)\n",
        "    axes[i, 2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJ8iDqqod6BB"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "For the final evaluation of the model, make sure to test performance on the left out category, `Canvas` to have a fair idea on how well the model generalises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "556T0aPid6BB"
      },
      "outputs": [],
      "source": [
        "test_module = CustomDataModule(loocv_splits=full_splits,\n",
        "                               current_material='Canvas',\n",
        "                               target_size=512,\n",
        "                               batch_size=4)\n",
        "\n",
        "test_module.setup()\n",
        "\n",
        "test_loader = test_module.val_dataloader()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# My Solution:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shape of the image batch: torch.Size([4, 3, 512, 512])\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, features=[64, 256, 512, 1024], latent_dims=16, verbose=False):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.features = features\n",
        "        self.latent_dims = latent_dims\n",
        "\n",
        "        # Define the encoder path (downsampling)\n",
        "        self.encoder_cnn = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels,\n",
        "                      out_channels=features[0], kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features[0], out_channels=features[1], kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features[1], out_channels=features[2], kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(\n",
        "                in_channels=features[2], out_channels=features[3], kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # self.encoder_flatten = nn.Flatten()\n",
        "        # self.bottleneck = nn.Sequential(\n",
        "        #     nn.Conv2d(features[3], features[3], kernel_size=1),\n",
        "        #     nn.ReLU(inplace=True)\n",
        "        # )\n",
        "\n",
        "        self.decoder_cnn = nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_channels=features[3], out_channels=features[2],\n",
        "                               kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=features[2], out_channels=features[1],\n",
        "                               kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=features[1], out_channels=features[0],\n",
        "                               kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(in_channels=features[0], out_channels=out_channels,\n",
        "                               kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Define the final convolution to reduce the number of channels to the number of classes:\n",
        "        # self.final_conv = nn.Conv2d(\n",
        "        #     in_channels=in_channels, out_channels=out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        original_size = x.size()[-2:]\n",
        "        \n",
        "        # ' Encoder\n",
        "        if self.verbose:\n",
        "            print(\"Input shape:\", x.shape)\n",
        "        \n",
        "        x = self.encoder_cnn(x)\n",
        "        if self.verbose:\n",
        "            print(\"Shape after encoder cnn:\", x.shape)\n",
        "\n",
        "        # x = self.bottleneck(x)\n",
        "        # if self.verbose:\n",
        "        #     print(\"Shape after bottleneck:\", x.shape)\n",
        "\n",
        "        # ' Decoder\n",
        "        x = self.decoder_cnn(x)\n",
        "        if self.verbose:\n",
        "            print(\"Shape after decoder cnn:\", x.shape)\n",
        "\n",
        "        # Force the model to output the same spatial dimensions as the input image:\n",
        "        # x = F.interpolate(x, size=original_size, mode='bilinear', align_corners=False)\n",
        "        # x = self.final_conv(x)\n",
        "        # if self.verbose:\n",
        "        #     print(\"Shape after final conv:\", x.shape)\n",
        "\n",
        "        # software because we are dealing with multi-class classification\n",
        "        # apply it to the first dimension (channels) because it represents the classes.\n",
        "        # Tensor shape: (batch_size, num_classes, height, width)\n",
        "        x = nn.Softmax(dim=1)(x)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"Shape after softmax:\", x.shape)\n",
        "\n",
        "        # x = torch.argmax(x, dim=1)\n",
        "        \n",
        "        # if self.verbose:\n",
        "        #     print(\"Shape after argmax:\", x.shape)\n",
        "\n",
        "        if self.verbose:\n",
        "            print()\n",
        "\n",
        "        return x\n",
        "\n",
        "    def enable_verbose(self, enabled):\n",
        "        self.verbose = enabled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Limit training dataset to only one material for simplicity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "limit_dataset = False # set to True to limit the dataset to one material for faster training\n",
        "\n",
        "if limit_dataset:\n",
        "    # limit the training dataset to only one material:\n",
        "    train_loader.dataset.dataframe = train_loader.dataset.dataframe[train_loader.dataset.dataframe['material'] == 'Glass']\n",
        "    print(\"number of material in training set:\", len(train_loader.dataset.dataframe['material'].unique()))\n",
        "\n",
        "    image_path = train_loader.dataset.dataframe.iloc[-1]['image_path']\n",
        "    image_annotation_path = train_loader.dataset.dataframe.iloc[-1]['annotation_rgb_path']\n",
        "\n",
        "    # from IPython.display import display, Image\n",
        "    # display(Image(filename=image_path))\n",
        "    # display(Image(filename=image_annotation_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tensor_similarity(outputs, targets):\n",
        "    # output's shape from network is (batch_size, num_classes, height, width), convert it to (batch_size, height, width) \n",
        "    outputs = torch.argmax(outputs, dim=1)\n",
        "    if outputs.shape != targets.shape:\n",
        "        raise ValueError(f\"Both tensors must have the same shape to compare their similarity. Got output's shape {outputs.shape} and target's shape {targets.shape} instead.\")\n",
        "    # Calculate the number of elements that are the same in both tensors\n",
        "    same_elements = torch.eq(outputs, targets)\n",
        "    # Calculate the similarity percentage\n",
        "    similarity_percentage = torch.mean(same_elements.float())\n",
        "    return similarity_percentage\n",
        "\n",
        "def train_epoch(model, train_loader, loss_fn, optimiser):\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "    # train_accuracy = []\n",
        "    train_similarity = []\n",
        "    for batch in train_loader:\n",
        "        images = batch['image'].to(DEVICE)\n",
        "        targets = batch['annotation'].to(DEVICE)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        # accuracy = calculate_accuracy(outputs, targets)\n",
        "        similarity = tensor_similarity(outputs, targets)\n",
        "        \n",
        "        # Backward pass:\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "        \n",
        "        train_loss.append(loss.detach().cpu().numpy())\n",
        "        # train_accuracy.append(accuracy.detach().cpu().numpy())\n",
        "        train_similarity.append(similarity.detach().cpu().numpy())\n",
        "        \n",
        "    return np.mean(train_loss), np.mean(train_similarity) # np.mean(train_accuracy)\n",
        "\n",
        "def test_epoch(model, test_loader, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    # test_accuracy = []\n",
        "    test_similarity = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            images = batch['image'].to(DEVICE)\n",
        "            targets = batch['annotation'].to(DEVICE)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            # accuracy = calculate_accuracy(outputs, targets)\n",
        "            similarity = tensor_similarity(outputs, targets)\n",
        "\n",
        "            test_loss.append(loss.detach().cpu().numpy())\n",
        "            # test_accuracy.append(accuracy.detach().cpu().numpy())\n",
        "            test_similarity.append(similarity.detach().cpu().numpy())\n",
        "    return np.mean(test_loss), np.mean(test_similarity) # np.mean(test_accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class WeightedCombinedLoss(nn.Module):\n",
        "    '''\n",
        "    Weighted Combined Loss:\n",
        "    - Cross Entropy Loss\n",
        "    - Dice Loss\n",
        "    \n",
        "    The class weights are used to balance the class distribution in the dataset. The Dice Loss is weighted by the class weights.\n",
        "    \n",
        "    The final loss is the sum of the Cross Entropy Loss and the weighted Dice Loss.\n",
        "    \n",
        "    The Dice Loss is calculated as follows:\n",
        "    - Convert the inputs to probabilities using the softmax function.\n",
        "    - Convert the targets to one-hot encoded format.\n",
        "    - Calculate the intersection and the union of the predictions and the targets.\n",
        "    - Calculate the Dice Loss using the formula: 1 - (2 * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
        "    \n",
        "    The combined loss is calculated as follows:\n",
        "    - CE = F.cross_entropy(inputs, targets, weight=class_weights, reduction='mean')\n",
        "    - Dice = dice_loss(F.softmax(inputs, dim=1).float(), F.one_hot(targets, inputs.size(1)).permute(0, 3, 1, 2).float(), smooth)\n",
        "    - Combined Loss = CE + weighted_dice\n",
        "    '''\n",
        "    def __init__(self, class_weights=None):\n",
        "        super(WeightedCombinedLoss, self).__init__()\n",
        "        self.class_weights = class_weights\n",
        "\n",
        "    def dice_loss(self, pred, target, smooth = 1.):\n",
        "        pred = pred.contiguous()\n",
        "        target = target.contiguous()    \n",
        "\n",
        "        intersection = (pred * target).sum(dim=2).sum(dim=2)\n",
        "        \n",
        "        loss = (1 - ((2. * intersection + smooth) / (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n",
        "        \n",
        "        return loss.mean()\n",
        "\n",
        "    def forward(self, inputs, targets, smooth=1):\n",
        "        '''\n",
        "        inputs => NxCxHxW\n",
        "        \n",
        "        targets => NxHxW\n",
        "        '''\n",
        "\n",
        "        # Cross Entropy Loss:\n",
        "        CE = F.cross_entropy(inputs, targets, weight=self.class_weights, reduction='mean')\n",
        "        \n",
        "        # Dice Loss:\n",
        "        dice_per_class = self.dice_loss(F.softmax(inputs, dim=1).float(),\n",
        "                         F.one_hot(targets, inputs.size(1)).permute(0, 3, 1, 2).float(),\n",
        "                         smooth)\n",
        "        weighted_dice = (dice_per_class * self.class_weights).mean()\n",
        "        \n",
        "        # Combined Loss:\n",
        "        combined_loss = CE + weighted_dice\n",
        "        # print(f\"CE: {CE}, Dice: {weighted_dice}, Combined: {combined_loss}\")\n",
        "        if combined_loss.isnan():\n",
        "            print(f\"CE: {CE}, Dice: {weighted_dice}, Combined: {combined_loss}\")\n",
        "        return combined_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameter values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "in_channels = train_loader.dataset[-1]['image'].shape[-3]\n",
        "# image_size = train_loader.dataset[-1]['image'].shape[-1] # square images so only one dimension is needed\n",
        "num_classes = len(class_names) + 1  # 16 damage classes + 1 background class\n",
        "out_channels = num_classes\n",
        "\n",
        "class_weights = torch.ones(num_classes)\n",
        "class_weights[0] = 0.1  # background class weight\n",
        "class_weights = class_weights.to(DEVICE)\n",
        "print(f\"Class weights for {len(class_weights)} classes: {class_weights.tolist()}\")\n",
        "loss_fn = WeightedCombinedLoss(class_weights=class_weights).to(DEVICE)\n",
        "\n",
        "lr = 1e-3\n",
        "latent_dims = 16\n",
        "momentum = 0.9\n",
        "weight_decay = 0.0\n",
        "dampening = 0\n",
        "\n",
        "print(f\"Initialising model with {in_channels} input channels and {out_channels} output channels with {latent_dims} latent dimensions.\")\n",
        "model = UNet(in_channels, out_channels, latent_dims=latent_dims).to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Hyperparameters optimisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ax.service.managed_loop import optimize\n",
        "from ax.plot.contour import plot_contour\n",
        "from ax.plot.trace import optimization_trace_single_method\n",
        "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
        "\n",
        "optimise_parameters = True\n",
        "\n",
        "if optimise_parameters:\n",
        "\n",
        "    def train_evaluate(parameterisation):\n",
        "        net = UNet(in_channels, out_channels, latent_dims=parameterisation.get('latent_dims')).to(DEVICE)\n",
        "        optimiser = optim.SGD(net.parameters(), lr=parameterisation.get('learning_rate'), momentum=parameterisation.get('momentum'), weight_decay=parameterisation.get('weight_decay'), dampening=parameterisation.get('dampening'))\n",
        "        train_epoch(net, train_loader, loss_fn, optimiser)\n",
        "        # val_loss, val_accuracy = test_epoch(net, val_loader, loss_fn)\n",
        "        prediction_correctness = torch.tensor([])\n",
        "        for batch in val_loader:\n",
        "            images = batch['image'].to(DEVICE)\n",
        "            targets = batch['annotation'].to(DEVICE)\n",
        "            outputs = net(images)\n",
        "            prediction_correctness.append(1 - tensor_similarity(outputs, targets)) # 100% - similarity\n",
        "        \n",
        "        prediction_correctness_mean = prediction_correctness.mean()\n",
        "        # print(f\"Parameters: {parameterisation}, Val Loss: {val_loss}, Val Accuracy: {val_accuracy}\")\n",
        "        # return {\"loss\": (val_loss, 0.0)}  # return a tuple of the mean and the standard deviation of the loss\n",
        "        print(f\"Parameters: {parameterisation}, Prediction Correctness: {prediction_correctness_mean}\")\n",
        "        return {\"accuracy\": (prediction_correctness_mean, 0.0)}\n",
        "    \n",
        "    print(\"Optimising hyperparameters..\")\n",
        "    parameters = [\n",
        "        {\"name\": \"learning_rate\", \"type\": \"range\", \"bounds\": [1e-5, 1e-1], \"log_scale\": True},\n",
        "        {\"name\": \"momentum\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
        "        {\"name\": \"weight_decay\", \"type\": \"range\", \"bounds\": [0.0, 2.0]},\n",
        "        {\"name\": \"dampening\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
        "        {\"name\": \"latent_dims\", \"type\": \"range\", \"bounds\": [2, 17]},\n",
        "    ]\n",
        "\n",
        "    best_parameters, values, experiment, optimiser_model = optimize(\n",
        "        parameters=parameters,\n",
        "        evaluation_function=train_evaluate,\n",
        "        objective_name=\"accuracy\",\n",
        "        total_trials=100,\n",
        "    )\n",
        "    \n",
        "    print()\n",
        "    print(\"best_parameters\", best_parameters)\n",
        "    print(\"values\", values)\n",
        "    print(\"experiment\", experiment)\n",
        "    print(\"model\", optimiser_model)\n",
        "\n",
        "    lr = round(best_parameters['learning_rate'], 5)\n",
        "    momentum = round(best_parameters['momentum'], 2)\n",
        "    weight_decay = round(best_parameters['weight_decay'], 5)\n",
        "    dampening = round(best_parameters['dampening'], 2)\n",
        "    latent_dims = best_parameters['latent_dims']\n",
        "\n",
        "    optimiser = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, dampening=dampening)\n",
        "\n",
        "    print(\"\\nFinished optimising hyperparameters.\")\n",
        "    print(\"Best parameters:\")\n",
        "    [print(f\"\\t{key}: {value}\") for key, value in best_parameters.items()]\n",
        "\n",
        "    init_notebook_plotting(offline=True)\n",
        "    best_objectives = np.array([[trial.objective_mean*100 for trial in experiment.trials.values()]])\n",
        "    data = optimization_trace_single_method(best_objectives, title=\"Optimization trace\", ylabel=\"loss\")\n",
        "    render(data)\n",
        "    \n",
        "else:\n",
        "    optimiser = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay, dampening=dampening)\n",
        "\n",
        "    print(\"Using default hyperparameters..\")\n",
        "    [print(f\"{key}: {value}\") for key, value in {'learning_rate': lr, 'latent_dims': latent_dims, 'weight_decay': weight_decay, 'momentum': momentum}.items()]\n",
        "\n",
        "\n",
        "print(\"########## Finished initialising model and training parameters. ##########\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_model = False # set to True to test the model correctness before training\n",
        "\n",
        "if test_model:\n",
        "    from torchsummary import summary\n",
        "    model.enable_verbose(True)\n",
        "    image_size = train_loader.dataset[-1]['image'].shape[-1] # square images so only one dimension is needed\n",
        "    summary(model, (in_channels, image_size, image_size))\n",
        "    # turn off verbose mode:\n",
        "    model.enable_verbose(False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_model_again = True # set to True to train the model again.\n",
        "save_model = True # set to True to save the model to github after training. train_model_again must be True if save_model is True.\n",
        "\n",
        "training_performance = {'train_loss': [], 'train_accuracy': []}\n",
        "validation_performance = {'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "if train_model_again:\n",
        "    num_epochs = 100\n",
        "    # model.enable_verbose(True)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_accuracy = train_epoch(model, train_loader, loss_fn, optimiser)\n",
        "        val_loss, val_accuracy = test_epoch(model, val_loader, loss_fn)\n",
        "    \n",
        "        if (epoch < 10) or ((epoch + 1) % 5 == 0):\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        training_performance['train_loss'].append(train_loss)\n",
        "        training_performance['train_accuracy'].append(train_accuracy)\n",
        "        validation_performance['val_loss'].append(val_loss)\n",
        "        validation_performance['val_accuracy'].append(val_accuracy)\n",
        "        print(f\"training performance: {training_performance}\")\n",
        "        print(f\"validation performance: {validation_performance}\")\n",
        "        \n",
        "    print(\"Training complete!\")\n",
        "    # Plot the training and validation losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    \n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(training_performance['train_loss'], label='Train Loss')\n",
        "    plt.plot(validation_performance['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(training_performance['train_accuracy'], label='Train Accuracy')\n",
        "    plt.plot(validation_performance['val_accuracy'], label='Val Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    \n",
        "    plt.title('Training and Validation Accuracies')\n",
        "    plt.legend()\n",
        "    \n",
        "    # save plot:\n",
        "    plt.savefig(f'img/training_results/losses_lr={lr}.svg')\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    if save_model:\n",
        "        # Save model locally:\n",
        "        torch.save(model.state_dict(), 'unet.pth')\n",
        "        # Save model to github:\n",
        "        !git add unet.pth\n",
        "        !git commit -m \"Add trained UNet model\"\n",
        "        !git push\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load model from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "load_model = False # set to True to load the model from github after training.\n",
        "\n",
        "if load_model:\n",
        "\n",
        "    import requests\n",
        "\n",
        "    def download_model(url, model_path):\n",
        "        r = requests.get(url, allow_redirects=True)\n",
        "        if r.status_code == 200:\n",
        "            # Override the model.pth file if it already exists:\n",
        "            with open(model_path, 'wb') as f:\n",
        "                f.write(r.content)\n",
        "            print(f\"Model downloaded to {model_path}\")\n",
        "            return model_path\n",
        "        else:\n",
        "            print(f\"Failed to download model. Status code: {r.status_code}\")\n",
        "            return None\n",
        "\n",
        "    model_url = 'https://raw.githubusercontent.com/JalalSayed1/Image-Damage-Classification/3baa846923307ae573f4fbecd7c8f00fc269fad4/unet.pth'\n",
        "    \n",
        "    model_path = download_model(model_url, 'unet.pth')\n",
        "    if model_path:\n",
        "        print(\"Model downloaded successfully!\")\n",
        "    else:\n",
        "        print(\"Failed to download model.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "model.load_state_dict(torch.load('unet.pth', map_location=DEVICE))\n",
        "model.eval()\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "def map_class_ids_to_rgb(input_tensor, id_to_rgb):\n",
        "    # Assuming input_tensor is of shape [1, H, W] and contains class IDs for each pixel\n",
        "    H, W = input_tensor.shape[1], input_tensor.shape[2]\n",
        "    \n",
        "    # Initialize the RGB image tensor\n",
        "    rgb_image = torch.zeros((3, H, W), dtype=torch.uint8)\n",
        "    \n",
        "    # Convert class ID tensor to a 2D array for easier processing\n",
        "    class_ids_2d = input_tensor.squeeze(0)\n",
        "    \n",
        "    # Iterate over each class ID in id_to_rgb mapping\n",
        "    for class_id, rgb in id_to_rgb.items():\n",
        "        # Create a mask for the current class ID\n",
        "        class_mask = class_ids_2d == class_id\n",
        "        \n",
        "        # Apply the mask to set the appropriate RGB values\n",
        "        for channel, color_value in enumerate(rgb):\n",
        "            rgb_image[channel][class_mask] = color_value\n",
        "\n",
        "    return rgb_image\n",
        "\n",
        "N = min(5, len(test_loader.dataset))\n",
        "print(f\"Number of examples to visualize: {N}\")\n",
        "\n",
        "# print(f\"example image shape: {test_loader.dataset[-1]['image'].shape}\")\n",
        "test_batches = [test_loader.dataset[i] for i in range(N)]\n",
        "\n",
        "for index, batch in enumerate(test_batches):\n",
        "    test_image = batch['image'].squeeze().to(DEVICE)\n",
        "    test_annotation = (batch['annotation'].squeeze()).to(DEVICE)\n",
        "    test_annotation_rgb = batch['annotation_rgb'].squeeze()\n",
        "    predicted_annotation = model(test_image)\n",
        "    \n",
        "    # print(f\"Shapes:\")\n",
        "    # print(f\"\\ttest_image: {test_image.shape}\")\n",
        "    # print(f\"\\ttest_annotation: {test_annotation.shape}\")\n",
        "    # print(f\"\\ttest_annotation_rgb: {test_annotation_rgb.shape}\")\n",
        "    # print(f\"\\tpredicted_annotation: {predicted_annotation.shape}\")\n",
        "    \n",
        "    predicted_annotation = torch.argmax(predicted_annotation, dim=0).unsqueeze(0)\n",
        "    # predicted_annotation_rgb = indices_to_rgb(predicted_annotation, id_to_rgb)\n",
        "    predicted_annotation_rgb = map_class_ids_to_rgb(predicted_annotation, id_to_rgb)\n",
        "    # print(f\"\\tpredicted_annotation_rgb: {predicted_annotation_rgb.shape}\")\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
        "    \n",
        "    similarity = round(tensor_similarity(predicted_annotation.unsqueeze(0), test_annotation.unsqueeze(0)).item() * 100, 2)\n",
        "    plt.suptitle(f\"Annotation Similarity: {similarity}%\", fontsize=24, ha='center', va='top', y=1.1)\n",
        "\n",
        "    titles = ['Image', 'Overlay', 'Predicted Overlay', 'Predicted']\n",
        "    for ax, title in zip(axes, titles):\n",
        "        ax.set_title(title, fontsize=24)\n",
        "        \n",
        "    test_image = denormalize(test_image.cpu().numpy().transpose((1, 2, 0)), mean, std)  # C, H, W -> H, W, C\n",
        "    test_annotation_rgb = test_annotation_rgb.numpy().transpose((1, 2, 0))  # C, H, W -> H, W, C\n",
        "    \n",
        "    predicted_annotation = predicted_annotation.cpu().numpy().squeeze() # remove singleton dimension\n",
        "    predicted_annotation_rgb = predicted_annotation_rgb.cpu().numpy().transpose((1, 2, 0)) # C, H, W -> H, W, C\n",
        "    \n",
        "    if similarity >= 0:\n",
        "        # Save the images to a file\n",
        "        test_image_pil = to_pil_image(test_image)\n",
        "        test_image_pil.save(f'img/predictions/test_results/{index}1.png')\n",
        "        test_annotation_rgb_pil = to_pil_image(test_annotation_rgb)\n",
        "        test_annotation_rgb_pil.save(f'img/predictions/test_results/{index}2.png')\n",
        "        predicted_annotation_rgb_pil = to_pil_image(predicted_annotation_rgb)\n",
        "        predicted_annotation_rgb_pil.save(f'img/predictions/test_results/{index}3.png')\n",
        "    \n",
        "    axes[0].imshow(test_image)\n",
        "    axes[0].axis('off')\n",
        "    \n",
        "    alpha_channel = np.all(test_annotation_rgb == [0, 0, 0], axis=-1)\n",
        "    test_annotation_rgba = np.dstack((test_annotation_rgb, np.where(alpha_channel, 0, 1)))\n",
        "    axes[1].imshow(test_image)\n",
        "    axes[1].imshow(test_annotation_rgba)\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    axes[2].imshow(predicted_annotation_rgb)\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    predicted_annotation_rgba = np.dstack((predicted_annotation_rgb, np.where(alpha_channel, 0, 1)))\n",
        "    axes[3].imshow(test_image)\n",
        "    axes[3].imshow(predicted_annotation_rgba)\n",
        "    axes[3].axis('off')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip freeze > requirements.txt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
